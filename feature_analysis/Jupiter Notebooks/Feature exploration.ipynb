{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .widget-label { min-width: 20ex !important; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1a23cd2f47485c9fc55b18216c93e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='show_data_params'), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5d42048a6a4b2da7ee3b8054dc2798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Prediction_Type', options=('Regression', 'Classification'), value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import json\n",
    "from ipywidgets import interact, interact_manual\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn import metrics\n",
    "import json\n",
    "import itertools as it\n",
    "display(HTML('''<style>\n",
    "    .widget-label { min-width: 20ex !important; }\n",
    "</style>'''))\n",
    "\n",
    "DATASETS_DIR = \"../datasets/\"\n",
    "\n",
    "#w = widgets.Checkbox(\n",
    "#    value=False,\n",
    "#    description='Check me',\n",
    "#    disabled=False,\n",
    "#    indent=False\n",
    "#)\n",
    "#display(w)\n",
    "#print(w.value)\n",
    "ranking = None\n",
    "df = None\n",
    "params = None\n",
    "df_text = None\n",
    "\n",
    "def get_data(params):\n",
    "    global df\n",
    "    global df_text\n",
    "    \n",
    "    df_text = pd.read_csv(DATASETS_DIR+\"id_to_text.csv\")\n",
    "    #pritn(\"Done loading post texts\")\n",
    "    \n",
    "    prepend_csv = \"prepend_done.csv\"\n",
    "    standalone_csv = \"standalone_done.csv\"\n",
    "\n",
    "    if params[\"title_prepend\"]:\n",
    "        df = load_wo_cols(DATASETS_DIR+prepend_csv, params)\n",
    "    else:\n",
    "        df = load_wo_cols(DATASETS_DIR+standalone_csv, params)\n",
    "\n",
    "    #print(\"Done loading features\")\n",
    "    if params[\"new_reactions\"]:\n",
    "        new_react = \"id_to_reactions_new.csv\"\n",
    "        df_reactions = pd.read_csv(DATASETS_DIR+new_react)\n",
    "        df = df.merge(df_reactions, left_on=\"post_id\", right_on=\"post_id\",\n",
    "                      validate=\"1:1\", suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\n",
    "\n",
    "    if params[\"norm\"] < 2:\n",
    "        df = df[df.columns.drop(\n",
    "            list(df.filter(regex=\"_abs\" if params[\"norm\"] == 1 else \"_norm\")))]\n",
    "\n",
    "    keys = [\"info\", \"yta\", \"nah\", \"esh\", \"nta\"]\n",
    "    weight = \"weighted_\" if params[\"weighted\"] else \"\"\n",
    "    values = [\"reactions_\"+weight+k.upper() for k in keys]\n",
    "    acros = dict(zip(keys, values))\n",
    "\n",
    "    dfs = []\n",
    "    if params[\"topics_separate\"] > 0:\n",
    "\n",
    "        topic_min = df[\"topic_nr\"].min()\n",
    "        topic_max = df[\"topic_nr\"].max()\n",
    "        #print(f\"Data split by topic ({topic_min}, {topic_max})\")\n",
    "\n",
    "        for i in range(topic_min, topic_max+1):\n",
    "            dfs.append(df.loc[df[\"topic_nr\"] == i])\n",
    "    else:\n",
    "        dfs = [df]\n",
    "\n",
    "\n",
    "    return dfs, acros\n",
    "\n",
    "\n",
    "def load_wo_cols(path, params, remove_cols=[], verbose=False):\n",
    "    cols_to_remove = [\"post_text\", \"Unnamed: 0\", \"Unnamed: 1\", \"Unnamed: 2\", \"Unnamed: 0.1\",\n",
    "                      \"Unnamed: 0.1.1\", \"liwc_post_id\", \"foundations_post_id\",\n",
    "                      \"foundations_title_post_id\", \"liwc_title_post_id\", \"post_created_utc\"]+remove_cols\n",
    "    metadata = [\"speaker_account_comment_karma\", \"post_num_comments\", \"speaker_account_age\",\n",
    "                \"speaker_account_link_karma\", \"post_ups\", \"post_downs\", \"post_score\", \"reactions_is_devil\", \"reactions_is_angel\",\"post_ratio\"]\n",
    "    # removed \"post_ratio\" from metadata b.c. used for weights\n",
    "\n",
    "    removed = []\n",
    "    df = pd.read_csv(path, nrows=10)\n",
    "    cols_to_read = list(df.columns)\n",
    "\n",
    "    # remove metadata\n",
    "    if params[\"wo_metadata\"]:\n",
    "        cols_to_remove = cols_to_remove+metadata\n",
    "\n",
    "    if params[\"new_reactions\"]:\n",
    "        cols_to_remove = cols_to_remove + \\\n",
    "            list(filter(lambda x: \"reaction\" in x and not \"reaction_is\" in x, cols_to_read))\n",
    "\n",
    "    # remove liwc\n",
    "    if not params[\"use_liwc\"]:\n",
    "        cols_to_remove = cols_to_remove + \\\n",
    "            list(filter(lambda x: \"liwc_\" in x, cols_to_read))\n",
    "\n",
    "    # remove moral foundations\n",
    "    if not params[\"use_mf\"]:\n",
    "        cols_to_remove = cols_to_remove + \\\n",
    "            list(filter(lambda x: \"foundations_\" in x, cols_to_read))\n",
    "\n",
    "    # post requirements setup\n",
    "    cols_to_remove = [\n",
    "        x for x in cols_to_remove if x not in list(params[\"requirements\"].keys())]\n",
    "\n",
    "    if verbose:\n",
    "        print(cols_to_read)\n",
    "    for col in cols_to_remove:\n",
    "        if col in cols_to_read:\n",
    "            cols_to_read.remove(col)\n",
    "            removed.append(col)\n",
    "\n",
    "    #print(f\"Removed {removed} from {path.split('/')[-1]}\")\n",
    "    #print(\"ONLY USING 10k lines\")\n",
    "    df = pd.read_csv(path, usecols=cols_to_read,)\n",
    "\n",
    "    # delte posts that don't meet requirements\n",
    "    nr_rows_pre_req = len(df)\n",
    "    for k, v in params[\"requirements\"].items():\n",
    "        df = df.loc[(df[k] >= v), :]\n",
    "    # remove cols required for \"requirements\"\n",
    "    if params[\"wo_metadata\"]:\n",
    "        to_drop = set(list(params[\"requirements\"].keys()))\n",
    "        in_list = set(list(df.columns))\n",
    "        will_drop = list(to_drop.intersection(in_list))\n",
    "        df = df.drop(columns=will_drop)\n",
    "        removed += will_drop\n",
    "        \n",
    "        \n",
    "    \n",
    "    # print(\n",
    "    #    f\"Removed {int(100*(nr_rows_pre_req-len(df))/len(df))}% due to requirements, Now {len(df)} posts remain.\")\n",
    "    # Check values in df\n",
    "    # df.describe().loc[['min','max']].to_csv(\"min_max.csv\",index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sampling(X_train, y_train, params, indices=[], verbose=False):\n",
    "    df_len_old = len(X_train)\n",
    "    if verbose:\n",
    "        print(f\"{params['sampling']}-sampling for {params['predict']}\")\n",
    "\n",
    "    if params[\"sampling\"] == \"none\":\n",
    "        X_train_ret = X_train\n",
    "        y_train_ret = y_train\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Original Y distribution on training set\")\n",
    "        _ = plt.hist(y_train, bins='auto')\n",
    "        plt.show()\n",
    "\n",
    "    if params[\"predict\"] == \"ratio\":\n",
    "        if params[\"sampling\"] == \"up\":\n",
    "            raise Exception(\"Upsampling with regression is not feasible☹️\")\n",
    "        elif params[\"sampling\"] == \"down\":\n",
    "            # downsampling\n",
    "            bucket_ranges = [x/10 for x in list(range(0, 11))]\n",
    "            bucket_counter = []\n",
    "\n",
    "            X_train_tmp = X_train\n",
    "            y_train_tmp = y_train.reshape((len(y_train), 1))\n",
    "            dummy_feat_name = [str(int) for int in range(X_train_tmp.shape[1])]\n",
    "            feat_names_to_sample = dummy_feat_name+[\"Y\"]\n",
    "            data_to_sample = np.append(X_train_tmp, y_train_tmp, 1)\n",
    "            df_to_sample = pd.DataFrame(\n",
    "                data_to_sample, columns=feat_names_to_sample)\n",
    "\n",
    "            # Get bucket sizes\n",
    "            for i in range(len(bucket_ranges)):\n",
    "                if bucket_ranges[i] == 1:\n",
    "                    continue\n",
    "                orig_size = len(df_to_sample.loc[(bucket_ranges[i] <= df_to_sample['Y']) & (\n",
    "                    df_to_sample['Y'] <= bucket_ranges[i+1])])\n",
    "                bucket_counter.append(orig_size)\n",
    "\n",
    "            # We only downsample buckets that are > 2* bucket mean => 2*bucket mean\n",
    "            bucket_max = int(np.mean(bucket_counter)*1.5)\n",
    "            for j in range(len(bucket_counter)):\n",
    "                if bucket_counter[j] > bucket_max:\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            f\"Bucket {bucket_ranges[j]}-{bucket_ranges[j+1]} has {bucket_counter[j]}>{bucket_max}\")\n",
    "                    df_bkt = df_to_sample.loc[(bucket_ranges[j] <= df_to_sample['Y']) & (\n",
    "                        df_to_sample['Y'] <= bucket_ranges[j+1])]\n",
    "                    df_bkt_smpl = df_bkt.sample(\n",
    "                        n=max(int(bucket_max),len(df_bkt)), replace=False, random_state=42)\n",
    "                    df_to_sample.loc[(bucket_ranges[j] <= df_to_sample['Y']) & (\n",
    "                        df_to_sample['Y'] <= bucket_ranges[j+1])] = df_bkt_smpl\n",
    "\n",
    "            df_to_sample = df_to_sample.dropna()\n",
    "            y_train = df_to_sample[\"Y\"]\n",
    "            df_to_sample = df_to_sample.drop(columns=[\"Y\"])\n",
    "\n",
    "            X_train = df_to_sample.to_numpy()\n",
    "            X_train_ret = X_train\n",
    "            y_train_ret = y_train\n",
    "\n",
    "    elif params[\"predict\"] == \"class\":\n",
    "        df_y = pd.DataFrame(data={\"Y\": y_train})\n",
    "\n",
    "        if len(indices) > 0:\n",
    "            if verbose:\n",
    "                print(f\"Using {len(indices)} indices\")\n",
    "        else:\n",
    "            indices = range(len(indices))\n",
    "\n",
    "        # Get list of indices for classes that are in the indices array\n",
    "        c0_idx = pd.Series(df_y.loc[df_y[\"Y\"] == 0].index.values)\n",
    "        c0_idx = c0_idx[c0_idx.isin(indices)]\n",
    "        c1_idx = pd.Series(df_y.loc[df_y[\"Y\"] == 1].index.values)\n",
    "        c1_idx = c1_idx[c1_idx.isin(indices)]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"    Y=0: {c0_idx.shape}\")\n",
    "            print(f\"    Y=1: {c1_idx.shape}\")\n",
    "\n",
    "        if params[\"sampling\"] == \"up\":\n",
    "            # upsample\n",
    "            if len(c0_idx) >= len(c1_idx):\n",
    "                n = len(c0_idx)\n",
    "                c1_idx_sampeled = c1_idx.sample(\n",
    "                    n=n, random_state=1, replace=len(c1_idx) < n).values\n",
    "                c0_idx_sampeled = c0_idx.values\n",
    "                if verbose:\n",
    "                    print(f\"Upsampling Y=1 with {n} samples\")\n",
    "\n",
    "            elif len(c0_idx) < len(c1_idx):\n",
    "                n = len(c1_idx)\n",
    "                c0_idx_sampeled = c0_idx.sample(\n",
    "                    n=n, random_state=1, replace=len(c0_idx) < n).values\n",
    "                c1_idx_sampeled = c1_idx.values\n",
    "                if verbose:\n",
    "                    print(f\"Upsampling Y=0 with {n} samples\")\n",
    "\n",
    "        elif params[\"sampling\"] == \"down\":\n",
    "            # downsample\n",
    "            if len(c0_idx) >= len(c1_idx):\n",
    "                n = len(c1_idx)\n",
    "                c0_idx_sampeled = c0_idx.sample(\n",
    "                    n=n, random_state=1, replace=len(c0_idx) < n).values\n",
    "                c1_idx_sampeled = c1_idx.values\n",
    "                if verbose:\n",
    "                    print(f\"Downsampling Y=0 with {n} samples\")\n",
    "            elif len(c0_idx) < len(c1_idx):\n",
    "                n = len(c0_idx)\n",
    "                c1_idx_sampeled = c1_idx.sample(\n",
    "                    n=n, random_state=1, replace=len(c1_idx) < n).values\n",
    "                c0_idx_sampeled = c0_idx.values\n",
    "                if verbose:\n",
    "                    print(f\"Downsampling Y=1 with {n} samples\")\n",
    "        else:\n",
    "            c0_idx_sampeled = c0_idx\n",
    "            c1_idx_sampeled = c1_idx\n",
    "\n",
    "        all_idx = np.concatenate((c0_idx_sampeled, c1_idx_sampeled), axis=0)\n",
    "\n",
    "        if verbose:\n",
    "            df_tmp = df_y.iloc[all_idx]\n",
    "            print(f\"   Y=0: {len(df_tmp.loc[df_tmp['Y']==0])}\")\n",
    "            print(f\"   Y=1: {len(df_tmp.loc[df_tmp['Y']==1])}\")\n",
    "\n",
    "        X_train_ret = X_train[all_idx, :]\n",
    "        y_train_ret = y_train[all_idx]\n",
    "\n",
    "    # print(df_len_old)\n",
    "    #print(f\"Removed/Added {int(100*(df_len_old-len(y_train_ret))/len(y_train_ret))}% due to Sampling, Now {len(y_train_ret)} posts remain.\")\n",
    "    return X_train_ret, y_train_ret\n",
    "\n",
    "\n",
    "def opposite_jdgmt(judg):\n",
    "\n",
    "    if \"NTA\" in judg:\n",
    "        rtn = judg.replace(\"NTA\", \"YTA\")\n",
    "    elif \"NAH\" in judg:\n",
    "        rtn = judg.replace(\"NAH\", \"ESH\")\n",
    "    elif \"YTA\" in judg:\n",
    "        rtn = judg.replace(\"YTA\", \"NTA\")\n",
    "    elif \"ESH\" in judg:\n",
    "        rtn = judg.replace(\"ESH\", \"NAH\")\n",
    "    elif \"INFO\" in judg:\n",
    "        rtn = judg\n",
    "\n",
    "    return rtn+\"_neg_vals\"\n",
    "\n",
    "\n",
    "# mapping is either \"clip\", meaning negative votes are just set to 0, or \"oppossite\", meaning we use the mapping table in \"opposite_jdgmt\"\n",
    "def map_negative_values(df, acros, mapping=\"clip\"):\n",
    "\n",
    "    if mapping == \"opposite\" or mapping == \"map\":\n",
    "        #print(\"Map = opposite\")\n",
    "        for k in acros.keys():\n",
    "            acr = acros[k]\n",
    "\n",
    "            if k == \"info\":\n",
    "                continue\n",
    "\n",
    "            # create temporary columns containing zeros and only negative votes for each vote type (except info)\n",
    "            df[acr+\"_neg_vals\"] = 0\n",
    "            df.loc[df[acr] < 0, acr+\"_neg_vals\"] = df[acr]*-1\n",
    "            df.loc[df[acr] < 0, acr] = 0\n",
    "\n",
    "        for k in acros.keys():\n",
    "            if k == \"info\":\n",
    "                continue\n",
    "            acr = acros[k]\n",
    "            # set negative values to 0 & add opposite judgement vote\n",
    "            df[acr] = df[acr] + df[opposite_jdgmt(acr)]\n",
    "\n",
    "    elif mapping == \"clip\":\n",
    "        #print(\"Map = clip\")\n",
    "        for k in acros.keys():\n",
    "            acr = acros[k]\n",
    "            df.loc[df[acr] < 0, acr] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_data_classes(df, acros, ratio=0.5, verbose=False, predict=\"class\", judgement_weighted=True, mapping=\"clip\"):\n",
    "    if verbose:\n",
    "        print(f\"df original shape {df.shape}\")\n",
    "\n",
    "    n_rows_old = len(df)\n",
    "\n",
    "    # Map negative judgements to opposing judgement, if we are not simply counting each comment as one vote (i.e. if judgement_weighted = True)\n",
    "    # i.e. YTA<->NTA, ESH<->NAH\n",
    "    if judgement_weighted:\n",
    "        df = map_negative_values(df, acros, mapping=mapping)\n",
    "\n",
    "    if predict == \"class\":\n",
    "        # We only look at YTA and NTA\n",
    "        df[\"YTA_ratio\"] = df[acros[\"yta\"]] / \\\n",
    "            (df[acros[\"info\"]] + df[acros[\"yta\"]] +\n",
    "             df[acros[\"nah\"]]+df[acros[\"esh\"]]+df[acros[\"nta\"]])\n",
    "\n",
    "        # drop all rows where the majority is not YTA or NTA\n",
    "        df = df.loc[((df[acros[\"yta\"]] > df[acros[\"info\"]]) & (df[acros[\"yta\"]] > df[acros[\"nah\"]]) & (df[acros[\"yta\"]] > df[acros[\"esh\"]])) | (\n",
    "            (df[acros[\"nta\"]] > df[acros[\"info\"]]) & (df[acros[\"nta\"]] > df[acros[\"nah\"]]) & (df[\"reactions_weighted_NTA\"] > df[acros[\"esh\"]]))]\n",
    "        if verbose:\n",
    "            print(f\"Drop all rows where majority is not YTA or NTA {df.shape}\")\n",
    "\n",
    "        # drop all rows that are not \"extreme\" enough\n",
    "        df = df.loc[(1-ratio <= df[\"YTA_ratio\"]) | (df[\"YTA_ratio\"] <= ratio)]\n",
    "\n",
    "        #print(\n",
    "        #    f\"Removed {int(100*( (n_rows_old-len(df)) / n_rows_old) )}% due to agreement ratio, Now {len(df)} posts remain.\")\n",
    "\n",
    "        # specifc classes & drop unnecesarry\n",
    "        # YTA = Class 1, NTA = class 0\n",
    "        df[\"Y\"] = np.where(df[acros[\"yta\"]] > df[acros[\"nta\"]], 1,  0)\n",
    "        smp_weights = None\n",
    "        if verbose:\n",
    "            print(df.shape)\n",
    "\n",
    "    elif predict == \"ratio\":\n",
    "        # Y = asshole ratio(AHR) = (YTA+ESH)/(YTA+ESH+NTA+NAH)\n",
    "        # drop posts w.o. votes\n",
    "        tmp = df[acros[\"yta\"]] + df[acros[\"nah\"]] + \\\n",
    "            df[acros[\"esh\"]]+df[acros[\"nta\"]]\n",
    "        tmp = tmp[tmp != 0]\n",
    "        tmp = (df[acros[\"yta\"]]+df[acros[\"esh\"]])/tmp\n",
    "        df[\"Y\"] = tmp\n",
    "\n",
    "        n_rows_old = len(df)\n",
    "        df = df.loc[(1-ratio <= df[\"Y\"]) | (df[\"Y\"] <= ratio)]\n",
    "        smp_weights = None\n",
    "        # print(\n",
    "        #    f\"Removed {int(100*(n_rows_old-len(df))/len(df))}% of posts b.c. not enough agreement. Now {df.shape}\")\n",
    "\n",
    "    if np.min(df[\"Y\"]) < 0 or np.max(df[\"Y\"]) > 1:\n",
    "        raise Exception(\"Y value should be in range [0,1]\")\n",
    "\n",
    "    # get list of all columns that contain uppercase vote acronym\n",
    "    vote_acroynms = list(filter(lambda x: any(\n",
    "        [acr.upper() in x for acr in list(acros.keys())]), list(df.columns)))\n",
    "    vote_acroynms += [\"post_id\"]\n",
    "    df = df.drop(columns=vote_acroynms)\n",
    "\n",
    "    if verbose:\n",
    "        print(df.shape)\n",
    "\n",
    "    X = df.drop(columns=[\"Y\"])\n",
    "    y = df[\"Y\"].to_numpy()\n",
    "\n",
    "    feat_name_lst = list(X.columns)\n",
    "\n",
    "    # scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    return X_scaled, y, feat_name_lst, None#smp_weights.to_numpy()\n",
    "\n",
    "\n",
    "def get_train_test_split(params, grid_search=False, verbose=False):\n",
    "    dfs, acros = get_data(params)\n",
    "\n",
    "    df = dfs[0]\n",
    "    if len(dfs) > 1:\n",
    "        print(\"MORE THAN 1 df\")\n",
    "\n",
    "    df_cpy = df.copy()\n",
    "    X, y, feat_name_lst,smp_weights = get_data_classes(df_cpy, ratio=params[\"ratio\"], acros=acros, predict=params[\"predict\"], judgement_weighted=params[\"weighted\"],\n",
    "                                           mapping=params[\"mapping\"], verbose=False)\n",
    "    if grid_search:\n",
    "        print(\"YOU SURE YOU WANT TO BE DOING THIS?\")\n",
    "        return X, y, feat_name_lst\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        range(len(X)), test_size=0.33, random_state=42)\n",
    "\n",
    "    X_train, y_train = sampling(\n",
    "        X[train], y[train], params, indices=train if params[\"predict\"] == \"class\" else [], verbose=False)\n",
    "\n",
    "    X_test = X[test, :]\n",
    "    y_test = y[test]\n",
    "\n",
    "    if params[\"random_y\"]:\n",
    "        # Sanity check, i.e. get results for random predition\n",
    "        #df[\"Y\"] = np.random.randint(0, 1001, size=len(df[\"Y\"]))/1000\n",
    "        \n",
    "        y_test_sum_old = np.sum(y_test[:len(y_test*0.5)])\n",
    "        np.random.shuffle(y_test)\n",
    "        y_test_sum_new = np.sum(y_test[:len(y_test*0.5)])\n",
    "        #if y_test_sum_old == y_test_sum_new:\n",
    "        #    print(\"Not truly random values\")\n",
    "        if verbose:\n",
    "            print(f\"USING RANDOM Y\\n Was {y_test_sum_old} Is {y_test_sum_new}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, feat_name_lst\n",
    "\n",
    "\n",
    "def get_clf_name(params, clf_type):\n",
    "    clf_name = clf_type\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, bool) and v:\n",
    "            clf_name += f\"_{k}\"\n",
    "        else:\n",
    "            clf_name += f\"_{k}={v}\"\n",
    "    return clf_name\n",
    "\n",
    "\n",
    "def get_metrics(y_test, y_pred, params, verbose=True):\n",
    "    if params[\"predict\"] == \"class\":\n",
    "        # testing score\n",
    "        f1_test = metrics.f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        acc_test = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"    Accuracy: {acc_test}\\n    F1: {f1_test}\")\n",
    "            print(classification_report(y_test, y_pred, target_names=[\n",
    "                \"Class 0: low AH\", \"Class 1: high AH\"]))\n",
    "        else:\n",
    "            return f1_test\n",
    "\n",
    "    elif params[\"predict\"] == \"ratio\":\n",
    "        mean_abs = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        mean_sqr = metrics.mean_squared_error(y_test, y_pred)\n",
    "        rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"    Mean absolute: {mean_abs}\\n    Mean squared: {mean_sqr}\\n    Root Mean Squared: {rmse}\")\n",
    "        else:\n",
    "            return mean_abs\n",
    "\n",
    "@interact(show_data_params=False, )\n",
    "def show_params(show_data_params):\n",
    "    global params\n",
    "    if params == None: \n",
    "        return\n",
    "    clf_type = \"Regression\" if params[\"predict\"]==\"ratio\" else \"Classification\"\n",
    "    if show_data_params:\n",
    "        print(f\"We are using these dataset parameters for {clf_type}:\\n{json.dumps(params, indent=4)}\")\n",
    "        \n",
    "@interact_manual(Prediction_Type=['Regression', \"Classification\"], Nr_features_to_show=(10, 169, 1))\n",
    "def training(Prediction_Type, Nr_features_to_show):\n",
    "    print(\"Changing Prediction_Type or Nr_features_to_show will retrain the model & generate new feature importance. This will take approx: 1-2min.\")\n",
    "    global ranking\n",
    "    global params\n",
    "    \n",
    "    regression_params = {        \n",
    "        \"norm\": 1, \n",
    "        \"weighted\": True,\n",
    "        \"title_prepend\": True,\n",
    "        \"sampling\": \"none\" ,\n",
    "        \"topics_separate\": False,\n",
    "        \"predict\": \"ratio\",\n",
    "        \"mapping\": \"opposite\", \"ratio\": 0.5,\n",
    "        \"wo_metadata\": True,\n",
    "        \"new_reactions\": False,\n",
    "        \"use_liwc\": True,\n",
    "        \"use_mf\": True,\n",
    "        \"requirements\": True,\n",
    "        \"random_y\":False\n",
    "    }\n",
    "    \n",
    "    classification_params = {        \n",
    "        \"norm\": 1, \n",
    "        \"weighted\": True,\n",
    "        \"title_prepend\": True,\n",
    "        \"sampling\": \"none\" ,\n",
    "        \"topics_separate\": False,\n",
    "        \"predict\": \"class\",\n",
    "        \"mapping\": \"opposite\", \"ratio\": 0.3,\n",
    "        \"wo_metadata\": True,\n",
    "        \"new_reactions\": False,\n",
    "        \"use_liwc\": True,\n",
    "        \"use_mf\": True,\n",
    "        \"requirements\": True,\n",
    "        \"random_y\":False\n",
    "    }\n",
    "    \n",
    "    post_requirements = {  # requirement: key >= value in post\n",
    "        \"post_num_comments\": 10,\n",
    "        \"post_score\": 10,\n",
    "        \"post_ratio\": 0.7,\n",
    "        }\n",
    "    \n",
    "\n",
    "    params = regression_params if Prediction_Type == \"Regression\" else classification_params\n",
    "    if params[\"requirements\"]:\n",
    "        params[\"requirements\"] = post_requirements\n",
    "    else:\n",
    "        params[\"requirements\"] = dict.fromkeys(post_requirements, 0)\n",
    "    \n",
    "    #Setup Model\n",
    "    xgboost = xgb.XGBClassifier(verbosity=0, random_state=42, use_label_encoder=False, ) if params[\"predict\"] == \"class\" else xgb.XGBRegressor(\n",
    "            verbosity=0, random_state=42, )\n",
    "    classifiers = (xgboost, \"xgboost\")\n",
    "    clf_name = get_clf_name(params, classifiers[1])\n",
    "    X_train, y_train, X_test, y_test, feat_name_lst = get_train_test_split(\n",
    "        params)\n",
    "\n",
    "    xgboost.fit(X_train, y_train, sample_weight=None)\n",
    "    y_pred = xgboost.predict(X_test)\n",
    "\n",
    "    is_regression = params[\"predict\"] == \"ratio\"\n",
    "\n",
    "    nr_samples = X_train.shape[0]\n",
    "    nr_features = X_train.shape[1]\n",
    "    complexity = nr_samples*nr_features\n",
    "    score = get_metrics(y_test, y_pred, params, verbose=False)\n",
    "\n",
    "    explainer = shap.explainers.Tree(xgboost, X_train)\n",
    "    shap_values = explainer(X_train)\n",
    "    key = \"class\" if params[\"predict\"] == \"class\" else \"ratio\"\n",
    "\n",
    "    #print(f'{\"SENSIBLE\" if \"sensible\" in params_i else \"BEST\"}, {key.upper()}')\n",
    "    #print(f'{\"F1\" if params_i[\"predict\"] == \"class\" else \"ME\" }: {score}')\n",
    "    #print(clf_name)\n",
    "    print(f'Got {\"F1=\" if params[\"predict\"] == \"class\" else \"ME=\"}{score}')\n",
    "    shap.summary_plot(shap_values, X_train, feature_names=feat_name_lst, max_display=Nr_features_to_show)\n",
    "\n",
    "    shap_df = pd.DataFrame(shap_values.values, columns=feat_name_lst)\n",
    "    vals = np.abs(shap_df.values).mean(0)\n",
    "    shap_importance = pd.DataFrame(list(zip(feat_name_lst, vals)), columns=['col_name', 'feature_importance_vals'])\n",
    "    shap_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "    \n",
    "    ranking = shap_importance[\"col_name\"].to_list()\n",
    "    \n",
    "    #display(HTML(shap_importance.to_html()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb88b1fc56124f9b86562678d16c8228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=True, description='Show_param_explanation'), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4be55c65f9f4c87b1f4365a01aae341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='feature_to_analyse', options=('foundations_WC', 'liwc_female', 'li…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "default_hist_bins = 50\n",
    "max_samples = 20\n",
    "\n",
    "Post_sample_from_bin_widget = widgets.IntSlider(\n",
    "    min=0, max=default_hist_bins-1, step=1, value=5,\n",
    "    style={'description_width': 'initial'}, layout = widgets.Layout(width='500px'))\n",
    "\n",
    "Nr_histogram_bins_widget = widgets.IntSlider(\n",
    "    min=10, max=500, step=10, value=default_hist_bins, \n",
    "    style={'description_width': 'initial'}, layout = widgets.Layout(width='500px'))\n",
    "\n",
    "Nr_samples_to_get_widget = widgets.IntSlider(\n",
    "    min=1, max=max_samples, step=1, value=1, \n",
    "    style={'description_width': 'initial'}, layout = widgets.Layout(width='500px'))\n",
    "\n",
    "def update_max_sample_idx(*args):\n",
    "    Post_sample_from_bin_widget.max = Nr_histogram_bins_widget.value\n",
    "\n",
    "    \n",
    "\n",
    "@interact(Show_param_explanation=True, )\n",
    "def show_param_explanation(Show_param_explanation):\n",
    "    if Show_param_explanation:\n",
    "        print(\"Parameter explantion:\")\n",
    "        print(\"    feature_to_analyse: Which feature we want to get a post_text sample from\")\n",
    "        print(\"    Amount_histo_bins: How many bins the histogram should use. The more bins the more fine grained.\")\n",
    "        print(\"    Bin_index_to_sample: From which histogram bin we want to get a post sample.\")\n",
    "        print(\"    Nr_samples_to_get: How many post samples we want to get. Max is 20.\")\n",
    "        print(\"    Reproducible_random_state: If we always want to get completely random samples within a bin or if they should be reproducible.\")\n",
    "    else:\n",
    "        print(\"Parameter explantion hidden\")\n",
    "\n",
    "def visualiser(feature_to_analyse, Amount_histo_bins, Bin_index_to_sample, Nr_samples_to_get, Reproducible_random_state):\n",
    "    global df_text\n",
    "    p = df[feature_to_analyse].plot(kind='hist', bins=Amount_histo_bins, color='blue')\n",
    "    p.patches[Bin_index_to_sample].set_color('orange')\n",
    "    plt.show()\n",
    "    \n",
    "    min_v = df[feature_to_analyse].min()\n",
    "    max_v = df[feature_to_analyse].max()\n",
    "    \n",
    "    bin_mins = np.linspace(min_v, max_v, num=Amount_histo_bins,endpoint=False)\n",
    "    sample_bin_min = bin_mins[Bin_index_to_sample]\n",
    "    sample_bin_max = max_v if Bin_index_to_sample+1 >= Amount_histo_bins else bin_mins[Bin_index_to_sample+1]\n",
    "    \n",
    "    df_sample = df.loc[((sample_bin_min<=df[feature_to_analyse]) & (df[feature_to_analyse]<=sample_bin_max))]\n",
    "    \n",
    "    nr_samples = len(df_sample)\n",
    "    print(f\"There are {nr_samples} posts in this bin.\")\n",
    "    Nr_samples_to_get_widget.max = min(nr_samples, max_samples)\n",
    "    \n",
    "    if nr_samples>=Nr_samples_to_get:\n",
    "        smpl = df_sample['post_id'].sample(n=Nr_samples_to_get, random_state=42 if Reproducible_random_state else None)\n",
    "        df_text_tmp = df_text.loc[df_text[\"post_id\"].isin(smpl)]\n",
    "        \n",
    "        for i in range(len(df_text_tmp)):\n",
    "            post_id = df_text_tmp.iloc[i][\"post_id\"]\n",
    "            post_text = df_text_tmp.iloc[i][\"post_text\"]\n",
    "            print(f\"\\nPOST ID: {post_id}\")\n",
    "            print(f\"POST TEXT:\\n{post_text}\")\n",
    "            print(\"-----------------------\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Not enough samples in this bucket. Wanted {Nr_samples_to_get}, but there are only {nr_samples}\") #this should never happen\n",
    "        \n",
    "Nr_histogram_bins_widget.observe(update_max_sample_idx, 'value')    \n",
    "\n",
    "interact(visualiser, feature_to_analyse=ranking, \n",
    "         Amount_histo_bins=Nr_histogram_bins_widget,\n",
    "         Bin_index_to_sample=Post_sample_from_bin_widget, \n",
    "         Nr_samples_to_get=Nr_samples_to_get_widget,\n",
    "         Reproducible_random_state=False);\n",
    "                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "tages": [
   "hide-input"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

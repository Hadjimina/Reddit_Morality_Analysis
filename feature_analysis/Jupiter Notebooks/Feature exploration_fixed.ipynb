{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .widget-label { min-width: 20ex !important; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcf4a0334af4a3f969f4ba204b09fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='show_data_params'), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1674d7c1339422a8666288fa877994a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Prediction_Type', options=('Regression', 'Classification'), value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import json\n",
    "from ipywidgets import interact, interact_manual\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn import metrics\n",
    "import json\n",
    "from datetime import date\n",
    "import itertools as it\n",
    "from sklearn.model_selection import KFold\n",
    "display(HTML('''<style>\n",
    "    .widget-label { min-width: 20ex !important; }\n",
    "</style>'''))\n",
    "\n",
    "DATASETS_DIR = \"../datasets/\"\n",
    "\n",
    "# w = widgets.Checkbox(\n",
    "#    value=False,\n",
    "#    description='Check me',\n",
    "#    disabled=False,\n",
    "#    indent=False\n",
    "# )\n",
    "# display(w)\n",
    "# print(w.value)\n",
    "ranking = None\n",
    "df = None\n",
    "params = None\n",
    "df_text = None\n",
    "\n",
    "\n",
    "def get_data(params):\n",
    "    global df\n",
    "    global df_text\n",
    "\n",
    "    print(\"1/3 Loading post texts started\")\n",
    "    df_text = pd.read_csv(DATASETS_DIR+\"id_to_text.csv\")\n",
    "\n",
    "    prepend_csv = \"prepend_done.csv\"\n",
    "    standalone_csv = \"standalone_done.csv\"\n",
    "\n",
    "    print(\"2/3 Loading post features started\")\n",
    "    if params[\"title_prepend\"]:\n",
    "\n",
    "        df = load_wo_cols(DATASETS_DIR+prepend_csv, params)\n",
    "    elif df:\n",
    "        df = load_wo_cols(DATASETS_DIR+standalone_csv, params)\n",
    "\n",
    "    if params[\"norm\"] < 2:\n",
    "        type_to_drop = \"_abs\" if params[\"norm\"] == 1 else \"_norm\"\n",
    "\n",
    "        df = df[df.columns.drop(list(df.filter(regex=type_to_drop)))]\n",
    "\n",
    "    keys = [\"info\", \"yta\", \"nah\", \"esh\", \"nta\"]\n",
    "    weight = \"weighted_\" if params[\"weighted\"] else \"\"\n",
    "    values = [\"reactions_\"+weight+k.upper() for k in keys]\n",
    "    acros = dict(zip(keys, values))\n",
    "\n",
    "    dfs = []\n",
    "    if params[\"topics_separate\"] > 0:\n",
    "\n",
    "        topic_min = df[\"topic_nr\"].min()\n",
    "        topic_max = df[\"topic_nr\"].max()\n",
    "        #print(f\"Data split by topic ({topic_min}, {topic_max})\")\n",
    "\n",
    "        for i in range(topic_min, topic_max+1):\n",
    "            dfs.append(df.loc[df[\"topic_nr\"] == i])\n",
    "    else:\n",
    "        dfs = [df]\n",
    "\n",
    "    return dfs, acros\n",
    "\n",
    "\n",
    "def load_wo_cols(path, params, remove_cols=[], verbose=False):\n",
    "    cols_to_remove = [\"post_text\", \"Unnamed: 0\", \"Unnamed: 1\", \"Unnamed: 2\", \"Unnamed: 0.1\",\n",
    "                      \"Unnamed: 0.1.1\", \"liwc_post_id\", \"foundations_post_id\",\n",
    "                      \"foundations_title_post_id\", \"liwc_title_post_id\", \"post_created_utc\"]+remove_cols\n",
    "    metadata = [\"speaker_account_comment_karma\", \"post_num_comments\", \"speaker_account_age\",\n",
    "                \"speaker_account_link_karma\", \"post_ups\", \"post_downs\", \"post_score\", \"reactions_is_devil\", \"reactions_is_angel\", \"post_ratio\"]\n",
    "    # removed \"post_ratio\" from metadata b.c. used for weights\n",
    "\n",
    "    removed = []\n",
    "    df = pd.read_csv(path, nrows=10)\n",
    "    cols_to_read = list(df.columns)\n",
    "\n",
    "    # remove metadata\n",
    "    if params[\"wo_metadata\"]:\n",
    "        cols_to_remove = cols_to_remove+metadata\n",
    "\n",
    "    # remove liwc\n",
    "    if not params[\"use_liwc\"]:\n",
    "        cols_to_remove = cols_to_remove + \\\n",
    "            list(filter(lambda x: \"liwc_\" in x, cols_to_read))\n",
    "\n",
    "    # remove moral foundations\n",
    "    if not params[\"use_mf\"]:\n",
    "        cols_to_remove = cols_to_remove + \\\n",
    "            list(filter(lambda x: \"foundations_\" in x, cols_to_read))\n",
    "\n",
    "    # post requirements setup\n",
    "    cols_to_remove = [\n",
    "        x for x in cols_to_remove if x not in list(params[\"requirements\"].keys())]\n",
    "\n",
    "    if verbose:\n",
    "        print(cols_to_read)\n",
    "    for col in cols_to_remove:\n",
    "        if col in cols_to_read:\n",
    "            cols_to_read.remove(col)\n",
    "            removed.append(col)\n",
    "\n",
    "    #print(f\"Removed {removed} from {path.split('/')[-1]}\")\n",
    "    #print(\"ONLY USING 10k lines\")\n",
    "    df = pd.read_csv(path, usecols=cols_to_read,) #TODO: FIXME\n",
    "\n",
    "    # delte posts that don't meet requirements\n",
    "    nr_rows_pre_req = len(df)\n",
    "    for k, v in params[\"requirements\"].items():\n",
    "        df = df.loc[(df[k] >= v), :]\n",
    "    # remove cols required for \"requirements\"\n",
    "    if params[\"wo_metadata\"]:\n",
    "        to_drop = set(list(params[\"requirements\"].keys()))\n",
    "        in_list = set(list(df.columns))\n",
    "        will_drop = list(to_drop.intersection(in_list))\n",
    "        df = df.drop(columns=will_drop)\n",
    "        removed += will_drop\n",
    "\n",
    "    # print(\n",
    "    #    f\"Removed {int(100*(nr_rows_pre_req-len(df))/len(df))}% due to requirements, Now {len(df)} posts remain.\")\n",
    "    # Check values in df\n",
    "    # df.describe().loc[['min','max']].to_csv(\"min_max.csv\",index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sampling(X_train, y_train, params, indices=[], verbose=False):\n",
    "    df_len_old = len(X_train)\n",
    "    pritn(\"HEEEEELLLLOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\")\n",
    "    print(f\"{params['sampling']}-sampling for {params['predict']}\")\n",
    "    if verbose:\n",
    "        print(f\"{params['sampling']}-sampling for {params['predict']}\")\n",
    "\n",
    "    if params[\"sampling\"] == \"none\":\n",
    "        X_train_ret = X_train\n",
    "        y_train_ret = y_train\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Original Y distribution on training set\")\n",
    "        _ = plt.hist(y_train, bins='auto')\n",
    "        plt.show()\n",
    "\n",
    "    if params[\"predict\"] == \"ratio\":\n",
    "        if params[\"sampling\"] == \"up\":\n",
    "            raise Exception(\"Upsampling with regression is not feasible☹️\")\n",
    "        elif params[\"sampling\"] == \"down\":\n",
    "            # downsampling\n",
    "            print(\"Downsampling\")\n",
    "            bucket_ranges = [x/10 for x in list(range(0, 11))]\n",
    "            bucket_counter = []\n",
    "\n",
    "            X_train_tmp = X_train\n",
    "            y_train_tmp = y_train.reshape((len(y_train), 1))\n",
    "            dummy_feat_name = [str(int) for int in range(X_train_tmp.shape[1])]\n",
    "            feat_names_to_sample = dummy_feat_name+[\"Y\"]\n",
    "            data_to_sample = np.append(X_train_tmp, y_train_tmp, 1)\n",
    "            df_to_sample = pd.DataFrame(\n",
    "                data_to_sample, columns=feat_names_to_sample)\n",
    "\n",
    "            # Get bucket sizes\n",
    "            for i in range(len(bucket_ranges)):\n",
    "                if bucket_ranges[i] == 1:\n",
    "                    continue\n",
    "                orig_size = len(df_to_sample.loc[(bucket_ranges[i] <= df_to_sample['Y']) & (\n",
    "                    df_to_sample['Y'] <= bucket_ranges[i+1])])\n",
    "                bucket_counter.append(orig_size)\n",
    "\n",
    "            # We only downsample buckets that are > 2* bucket mean => 2*bucket mean\n",
    "            bucket_max = int(np.mean(bucket_counter)*1.5)\n",
    "            for j in range(len(bucket_counter)):\n",
    "                if bucket_counter[j] > bucket_max:\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            f\"Bucket {bucket_ranges[j]}-{bucket_ranges[j+1]} has {bucket_counter[j]}>{bucket_max}\")\n",
    "                    df_bkt = df_to_sample.loc[(bucket_ranges[j] <= df_to_sample['Y']) & (\n",
    "                        df_to_sample['Y'] <= bucket_ranges[j+1])]\n",
    "                    df_bkt_smpl = df_bkt.sample(\n",
    "                        n=max(int(bucket_max), len(df_bkt)), replace=False, random_state=42)\n",
    "                    df_to_sample.loc[(bucket_ranges[j] <= df_to_sample['Y']) & (\n",
    "                        df_to_sample['Y'] <= bucket_ranges[j+1])] = df_bkt_smpl\n",
    "\n",
    "            df_to_sample = df_to_sample.dropna()\n",
    "            y_train = df_to_sample[\"Y\"]\n",
    "            df_to_sample = df_to_sample.drop(columns=[\"Y\"])\n",
    "\n",
    "            X_train = df_to_sample.to_numpy()\n",
    "            X_train_ret = X_train\n",
    "            y_train_ret = y_train\n",
    "\n",
    "    elif params[\"predict\"] == \"class\":\n",
    "        df_y = pd.DataFrame(data={\"Y\": y_train})\n",
    "\n",
    "        if len(indices) > 0:\n",
    "            if verbose:\n",
    "                print(f\"Using {len(indices)} indices\")\n",
    "        else:\n",
    "            indices = range(len(indices))\n",
    "\n",
    "        # Get list of indices for classes that are in the indices array\n",
    "        c0_idx = pd.Series(df_y.loc[df_y[\"Y\"] == 0].index.values)\n",
    "        c0_idx = c0_idx[c0_idx.isin(indices)]\n",
    "        c1_idx = pd.Series(df_y.loc[df_y[\"Y\"] == 1].index.values)\n",
    "        c1_idx = c1_idx[c1_idx.isin(indices)]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"    Y=0: {c0_idx.shape}\")\n",
    "            print(f\"    Y=1: {c1_idx.shape}\")\n",
    "\n",
    "        if params[\"sampling\"] == \"up\":\n",
    "            # upsample\n",
    "            if len(c0_idx) >= len(c1_idx):\n",
    "                n = len(c0_idx)\n",
    "                c1_idx_sampeled = c1_idx.sample(\n",
    "                    n=n, random_state=1, replace=len(c1_idx) < n).values\n",
    "                c0_idx_sampeled = c0_idx.values\n",
    "                if verbose:\n",
    "                    print(f\"Upsampling Y=1 with {n} samples\")\n",
    "\n",
    "            elif len(c0_idx) < len(c1_idx):\n",
    "                n = len(c1_idx)\n",
    "                c0_idx_sampeled = c0_idx.sample(\n",
    "                    n=n, random_state=1, replace=len(c0_idx) < n).values\n",
    "                c1_idx_sampeled = c1_idx.values\n",
    "                if verbose:\n",
    "                    print(f\"Upsampling Y=0 with {n} samples\")\n",
    "\n",
    "        elif params[\"sampling\"] == \"down\":\n",
    "            # downsample\n",
    "            if len(c0_idx) >= len(c1_idx):\n",
    "                n = len(c1_idx)\n",
    "                c0_idx_sampeled = c0_idx.sample(\n",
    "                    n=n, random_state=1, replace=len(c0_idx) < n).values\n",
    "                c1_idx_sampeled = c1_idx.values\n",
    "                if verbose:\n",
    "                    print(f\"Downsampling Y=0 with {n} samples\")\n",
    "            elif len(c0_idx) < len(c1_idx):\n",
    "                n = len(c0_idx)\n",
    "                c1_idx_sampeled = c1_idx.sample(\n",
    "                    n=n, random_state=1, replace=len(c1_idx) < n).values\n",
    "                c0_idx_sampeled = c0_idx.values\n",
    "                if verbose:\n",
    "                    print(f\"Downsampling Y=1 with {n} samples\")\n",
    "        else:\n",
    "            c0_idx_sampeled = c0_idx\n",
    "            c1_idx_sampeled = c1_idx\n",
    "\n",
    "        all_idx = np.concatenate((c0_idx_sampeled, c1_idx_sampeled), axis=0)\n",
    "\n",
    "        if verbose:\n",
    "            df_tmp = df_y.iloc[all_idx]\n",
    "            print(f\"   Y=0: {len(df_tmp.loc[df_tmp['Y']==0])}\")\n",
    "            print(f\"   Y=1: {len(df_tmp.loc[df_tmp['Y']==1])}\")\n",
    "\n",
    "        X_train_ret = X_train[all_idx, :]\n",
    "        y_train_ret = y_train[all_idx]\n",
    "\n",
    "    # print(df_len_old)\n",
    "    #print(f\"Removed/Added {int(100*(df_len_old-len(y_train_ret))/len(y_train_ret))}% due to Sampling, Now {len(y_train_ret)} posts remain.\")\n",
    "    return X_train_ret, y_train_ret\n",
    "\n",
    "def opposite_jdgmt(judg):\n",
    "    if \"NTA\" in judg:\n",
    "        rtn = judg.replace(\"NTA\", \"YTA\")\n",
    "    elif \"NAH\" in judg:\n",
    "        rtn = judg.replace(\"NAH\", \"ESH\")\n",
    "    elif \"YTA\" in judg:\n",
    "        rtn = judg.replace(\"YTA\", \"NTA\")\n",
    "    elif \"ESH\" in judg:\n",
    "        rtn = judg.replace(\"ESH\", \"NAH\")\n",
    "    elif \"INFO\" in judg:\n",
    "        rtn = judg\n",
    "\n",
    "    return rtn+\"_neg_vals\"\n",
    "\n",
    "\n",
    "# mapping is either \"clip\", meaning negative votes are just set to 0, or \"oppossite\", meaning we use the mapping table in \"opposite_jdgmt\"\n",
    "def map_negative_values(df, acros, mapping=\"clip\"):\n",
    "\n",
    "    if mapping == \"opposite\" or mapping == \"map\":\n",
    "        print(\"Map = opposite\")\n",
    "        for k in acros.keys():\n",
    "            acr = acros[k]\n",
    "            df[acr] = df[acr] + (-1*df[opposite_jdgmt(acr)])\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_data_classes(df, acros, ratio=0.5, verbose=False, predict=\"class\", judgement_weighted=True, mapping=\"clip\", save_excerpt=False):\n",
    "    if verbose:\n",
    "        print(f\"df original shape {df.shape}\")\n",
    "\n",
    "    n_rows_old = len(df)\n",
    "\n",
    "    # Map negative judgements to opposing judgement, if we are not simply counting each comment as one vote (i.e. if judgement_weighted = True)\n",
    "    # i.e. YTA<->NTA, ESH<->NAH\n",
    "    if judgement_weighted:\n",
    "        df = map_negative_values(df, acros, mapping=mapping)\n",
    "\n",
    "    if predict == \"class\":\n",
    "        # We only look at YTA and NTA\n",
    "        df[\"YTA_ratio\"] = df[acros[\"yta\"]] / \\\n",
    "            (df[acros[\"info\"]] + df[acros[\"yta\"]] +\n",
    "             df[acros[\"nah\"]]+df[acros[\"esh\"]]+df[acros[\"nta\"]])\n",
    "\n",
    "        # drop all rows where the majority is not YTA or NTA\n",
    "        df = df.loc[((df[acros[\"yta\"]] > df[acros[\"info\"]]) & (df[acros[\"yta\"]] > df[acros[\"nah\"]]) & (df[acros[\"yta\"]] > df[acros[\"esh\"]])) | (\n",
    "            (df[acros[\"nta\"]] > df[acros[\"info\"]]) & (df[acros[\"nta\"]] > df[acros[\"nah\"]]) & (df[\"reactions_weighted_NTA\"] > df[acros[\"esh\"]]))]\n",
    "        if verbose:\n",
    "            print(f\"Drop all rows where majority is not YTA or NTA {df.shape}\")\n",
    "\n",
    "        # drop all rows that are not \"extreme\" enough\n",
    "        df = df.loc[(1-ratio <= df[\"YTA_ratio\"]) | (df[\"YTA_ratio\"] <= ratio)]\n",
    "\n",
    "        # specifc classes & drop unnecesarry\n",
    "        # YTA = Class 1, NTA = class 0\n",
    "        df[\"Y\"] = np.where(df[acros[\"yta\"]] > df[acros[\"nta\"]], 1,  0)\n",
    "        smp_weights = None\n",
    "        if verbose:\n",
    "            print(df.shape)\n",
    "\n",
    "    elif predict == \"ratio\":\n",
    "        # Y = asshole ratio(AHR) = (YTA+ESH)/(YTA+ESH+NTA+NAH)\n",
    "        # drop posts w.o. votes\n",
    "        tmp = df[acros[\"yta\"]] + df[acros[\"nah\"]] + \\\n",
    "            df[acros[\"esh\"]]+df[acros[\"nta\"]]\n",
    "        tmp = tmp[tmp != 0]\n",
    "        tmp = (df[acros[\"yta\"]]+df[acros[\"esh\"]])/tmp\n",
    "        df[\"Y\"] = tmp\n",
    "        df = df[df[\"Y\"].notna()]\n",
    "\n",
    "        n_rows_old = len(df)\n",
    "        df = df.loc[(1-ratio <= df[\"Y\"]) | (df[\"Y\"] <= ratio)]  # MODIFY ME\n",
    "        smp_weights = None\n",
    "\n",
    "    if np.min(df[\"Y\"]) < 0 or np.max(df[\"Y\"]) > 1:\n",
    "        raise Exception(\"Y value should be in range [0,1]\")\n",
    "\n",
    "    # get Y values for specific posts\n",
    "    # change me if you want different y_values\n",
    "    posts_to_get = [\"eq3k7y\",\"mj8a47\",\"gcmhy1\",\"kcf1e0\",\"bs50ps\",\"gcti52\",\"cmkl5l\",\"bd0ww1\",\"b6uiz7\",\"bt8mm5\",\"blexov\",\"aos6vn\",\"hv6xro\",\"ggyb2v\",\"b1cbcz\",\"dn6075\",\"bu2kf5\",\"hql2q4\",\"cjudzm\",\"j0do3l\"]\n",
    "    y_vals = df[df['post_id'].isin(posts_to_get)][[\"Y\",\"post_id\",acros[\"yta\"], acros[\"nta\"], acros[\"nah\"], acros[\"esh\"]]]\n",
    "    y_vals.to_excel(\"y_vals.xlsx\")\n",
    "\n",
    "    # get list of all columns that contain uppercase vote acronym\n",
    "    vote_acroynms = list(filter(lambda x: any(\n",
    "        [acr.upper() in x for acr in list(acros.keys())]), list(df.columns)))\n",
    "    vote_acroynms += [\"post_id\"]\n",
    "    df = df.drop(columns=vote_acroynms)\n",
    "\n",
    "    if verbose:\n",
    "        print(df.shape)\n",
    "\n",
    "    if save_excerpt:\n",
    "        df.head(2).to_csv(\"../post_modification/data/prepend_done_trained_feats.csv\", index=False)\n",
    "        \n",
    "    X = df.drop(columns=[\"Y\"])\n",
    "    y = df[\"Y\"].to_numpy()\n",
    "\n",
    "    feat_name_lst = list(X.columns)\n",
    "\n",
    "    # scalings\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    return X_scaled, y, feat_name_lst, None  # smp_weights.to_numpy()\n",
    "\n",
    "\n",
    "def get_train_test_split(params, grid_search=False, verbose=False):\n",
    "    dfs, acros = get_data(params)\n",
    "\n",
    "    df = dfs[0]\n",
    "    if len(dfs) > 1:\n",
    "        print(\"MORE THAN 1 df\")\n",
    "\n",
    "    df_cpy = df.copy()\n",
    "    X, y, feat_name_lst, smp_weights = get_data_classes(df_cpy, ratio=params[\"ratio\"], acros=acros, predict=params[\"predict\"], judgement_weighted=params[\"weighted\"],\n",
    "                                                        mapping=params[\"mapping\"], verbose=False)\n",
    "    if grid_search:\n",
    "        print(\"YOU SURE YOU WANT TO BE DOING THIS?\")\n",
    "        return X, y, feat_name_lst\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        range(len(X)), test_size=0.33,)  # use ranomd_state=42 for repeatable values\n",
    "\n",
    "    X_train, y_train = sampling(\n",
    "        X[train], y[train], params, indices=train if params[\"predict\"] == \"class\" else [], verbose=False)\n",
    "\n",
    "    X_test = X[test, :]\n",
    "    y_test = y[test]\n",
    "\n",
    "    if params[\"random_y\"]:\n",
    "        # Sanity check, i.e. get results for random predition\n",
    "        #df[\"Y\"] = np.random.randint(0, 1001, size=len(df[\"Y\"]))/1000\n",
    "\n",
    "        y_test_sum_old = np.sum(y_test[:len(y_test*0.5)])\n",
    "        np.random.shuffle(y_test)\n",
    "        y_test_sum_new = np.sum(y_test[:len(y_test*0.5)])\n",
    "        # if y_test_sum_old == y_test_sum_new:\n",
    "        #    print(\"Not truly random values\")\n",
    "        if verbose:\n",
    "            print(f\"USING RANDOM Y\\n Was {y_test_sum_old} Is {y_test_sum_new}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, feat_name_lst, test\n",
    "\n",
    "\n",
    "def get_clf_name(params, clf_type):\n",
    "    clf_name = clf_type\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, bool) and v:\n",
    "            clf_name += f\"_{k}\"\n",
    "        else:\n",
    "            clf_name += f\"_{k}={v}\"\n",
    "    return clf_name\n",
    "\n",
    "\n",
    "def get_metrics(y_test, y_pred, params, verbose=True):\n",
    "    if params[\"predict\"] == \"class\":\n",
    "        # testing score\n",
    "        f1_test = metrics.f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        acc_test = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"    Accuracy: {acc_test}\\n    F1: {f1_test}\")\n",
    "            print(classification_report(y_test, y_pred, target_names=[\n",
    "                \"Class 0: low AH\", \"Class 1: high AH\"]))\n",
    "        return f1_test\n",
    "\n",
    "    elif params[\"predict\"] == \"ratio\":\n",
    "        mean_abs = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        mean_sqr = metrics.mean_squared_error(y_test, y_pred)\n",
    "        rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"    Mean absolute: {mean_abs}\\n    Mean squared: {mean_sqr}\\n    Root Mean Squared: {rmse}\")\n",
    "        return mean_abs\n",
    "\n",
    "\n",
    "@interact(show_data_params=False, )\n",
    "def show_params(show_data_params):\n",
    "    global params\n",
    "    if params == None:\n",
    "        return\n",
    "    clf_type = \"Regression\" if params[\"predict\"] == \"ratio\" else \"Classification\"\n",
    "    if show_data_params:\n",
    "        print(\n",
    "            f\"We are using these dataset parameters for {clf_type}:\\n{json.dumps(params, indent=4)}\")\n",
    "\n",
    "\n",
    "@interact_manual(Prediction_Type=['Regression', \"Classification\"], Nr_features_to_show=(10, 169, 1), ignore_certain_feats=[True, False])\n",
    "def training(Prediction_Type, Nr_features_to_show, ignore_certain_feats,):\n",
    "    print(\"Changing Prediction_Type or Nr_features_to_show will retrain the model & generate new feature importance. This will take approx: 1-4min.\")\n",
    "    global ranking\n",
    "    global params\n",
    "\n",
    "    regression_params = {\n",
    "        \"norm\": 1,\n",
    "        \"weighted\": True, \n",
    "        \"title_prepend\": True,\n",
    "        \"sampling\": \"none\",#this param no longer does anything\n",
    "        \"topics_separate\": False,\n",
    "        \"predict\": \"ratio\",\n",
    "        \"mapping\": \"opposite\", \"ratio\": 0.5,\n",
    "        \"wo_metadata\": True,\n",
    "        \"use_liwc\": True,\n",
    "        \"use_mf\": True,\n",
    "        \"requirements\": True,\n",
    "        \"random_y\": False\n",
    "    }\n",
    "\n",
    "    classification_params = {\n",
    "        \"norm\": 1,\n",
    "        \"weighted\": True,\n",
    "        \"title_prepend\": True,\n",
    "        \"sampling\": \"none\",\n",
    "        \"topics_separate\": False,\n",
    "        \"predict\": \"class\",\n",
    "        \"mapping\": \"opposite\",\n",
    "        \"ratio\": 0.3,\n",
    "        \"wo_metadata\": True,\n",
    "        \"use_liwc\": True,\n",
    "        \"use_mf\": True,\n",
    "        \"requirements\": True,\n",
    "        \"random_y\": False\n",
    "    }\n",
    "\n",
    "    post_requirements = {  # requirement: key >= value in post\n",
    "        \"post_num_comments\": 10,\n",
    "        \"post_score\": 10,\n",
    "        \"post_ratio\": 0.7,\n",
    "    }\n",
    "\n",
    "    features_to_ignore = ['foundations_SemiC', 'foundations_Quote', 'foundations_Colon', 'foundations_Sixltr', 'foundations_Parenth', 'foundations_Dash', \"topic_nr\"]\n",
    "    params = regression_params if Prediction_Type == \"Regression\" else classification_params\n",
    "    if params[\"requirements\"]:\n",
    "        params[\"requirements\"] = post_requirements\n",
    "    else:\n",
    "        params[\"requirements\"] = dict.fromkeys(post_requirements, 0)\n",
    "\n",
    "    # Setup Model\n",
    "    # scale_pos_weight\n",
    "    hyper_params_reg = {'learning_rate': 0.2257949690293526,\n",
    "                        'max_depth': 5, 'min_child_weight': 4, 'n_estimators': 180}\n",
    "    # hyper_params_reg_new =\n",
    "    hyper_params_clf = {'learning_rate': 1.0835557461256857, 'max_depth': 5,\n",
    "                        'min_child_weight': 3, 'n_estimators': 420, }  # 'scale_pos_weight':18372/5581}\n",
    "    clf = xgb.XGBClassifier(verbosity=0, random_state=42, use_label_encoder=False, **hyper_params_clf) if params[\"predict\"] == \"class\" else xgb.XGBRegressor(\n",
    "    verbosity=0, random_state=42, **hyper_params_reg)\n",
    "\n",
    "    #hyper_params_reg_rf = {'max_depth': 5, 'min_samples_leaf': 1,\n",
    "    #                       'min_samples_split': 2, 'n_estimators': 180}\n",
    "    #clf = RandomForestClassifier(random_state=42) if params[\"predict\"] == \"class\" else RandomForestRegressor(\n",
    "    #    random_state=42, **hyper_params_reg_rf)\n",
    "    classifiers = (clf, \"xgboost\")\n",
    "    #classifiers = (clf, \"rf\")\n",
    "    clf_name = get_clf_name(params, classifiers[1])\n",
    "\n",
    "    dfs, acros = get_data(params)\n",
    "    cur_df_0 = dfs[0]\n",
    "\n",
    "    if ignore_certain_feats:\n",
    "        print(\"DROPPING GENERATED FEATURES:\")\n",
    "        print(features_to_ignore)\n",
    "        cur_df_0 = cur_df_0.drop(columns=features_to_ignore)\n",
    "        \n",
    "\n",
    "    X, y, feat_name_lst, smp_weights = get_data_classes(cur_df_0, ratio=params[\"ratio\"], acros=acros, predict=params[\"predict\"], judgement_weighted=params[\"weighted\"],\n",
    "                                                        mapping=params[\"mapping\"], verbose=False)\n",
    "\n",
    "    print(f\"X SHAPE {X.shape}\")\n",
    "\n",
    "    list_shap_values = list()\n",
    "    list_test_sets = list()\n",
    "    list_scores = list()\n",
    "    splits = 4\n",
    "\n",
    "    if params[\"predict\"] == \"class\":\n",
    "        X = X[:-1]\n",
    "    if len(X) % splits != 0:\n",
    "        print(\"NOT DIVISIBLE BY SPLIT. WILL GET ERROR IN SHAP.\"+str(len(X)))\n",
    "\n",
    "    kf = KFold(n_splits=splits, shuffle=True)\n",
    "    list_shap_values = list()\n",
    "    list_test_sets = list()\n",
    "    print(f\"Model Training started. We will train {splits} models\")\n",
    "    counter = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training model {counter+1}/{splits}\")\n",
    "        counter += 1\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        X_train = pd.DataFrame(X_train, columns=feat_name_lst)\n",
    "        X_test = pd.DataFrame(X_test, columns=feat_name_lst)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        if ignore_certain_feats:\n",
    "            today = date.today()\n",
    "            d1 = today.strftime(\"%d.%m.%Y\")\n",
    "            clf.save_model(f\"../post_modification/data/{classifiers[1]}{d1}.json\")\n",
    "\n",
    "        score = get_metrics(y_test, y_pred, params, verbose=False)\n",
    "        list_scores.append(score)\n",
    "        explainer = shap.TreeExplainer(clf)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        # for each iteration we save the test_set index and the shap_values\n",
    "        list_shap_values.append(shap_values)\n",
    "        list_test_sets.append(test_index)\n",
    "\n",
    "    # combining results from all iterations\n",
    "    test_set = list_test_sets[0]\n",
    "    shap_values = np.array(list_shap_values[0])\n",
    "\n",
    "    for i in range(1, len(list_test_sets)):\n",
    "        test_set = np.concatenate((test_set, list_test_sets[i]), axis=0)\n",
    "        shap_values = np.concatenate(\n",
    "            (shap_values, np.array(list_shap_values[i])), axis=0)\n",
    "    # bringing back variable names\n",
    "    X_test = pd.DataFrame(X[test_set], columns=feat_name_lst)\n",
    "    print(f\"Mean score: {np.mean(np.array(list_scores))}\")\n",
    "    if ignore_certain_feats:\n",
    "        print(\"Updated model and feature csv extract in post modification tools\")\n",
    "    shap.summary_plot(shap_values, X_test, max_display=Nr_features_to_show,show=False)\n",
    "    plt.savefig('summary.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "    #key = \"class\" if params[\"predict\"] == \"class\" else \"ratio\"\n",
    "\n",
    "    #print(f'{\"SENSIBLE\" if \"sensible\" in params_i else \"BEST\"}, {key.upper()}')\n",
    "    #print(f'{\"F1\" if params_i[\"predict\"] == \"class\" else \"ME\" }: {score}')\n",
    "    # print(clf_name)\n",
    "\n",
    "    #shap.summary_plot(shap_values, X_train, feature_names=feat_name_lst, max_display=Nr_features_to_show)\n",
    "    \n",
    "    shap_df = pd.DataFrame(shap_values, columns=feat_name_lst)\n",
    "    vals = np.abs(shap_df.values).mean(0)\n",
    "    shap_importance = pd.DataFrame(list(zip(feat_name_lst, vals)), columns=[\n",
    "                                   'col_name', 'feature_importance_vals'])\n",
    "    shap_importance.sort_values(\n",
    "        by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "    ranking = shap_importance[\"col_name\"].to_list()\n",
    "\n",
    "    # display(HTML(shap_importance.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter Notebooks/Feature exploration_fixed.ipynb Cell 2'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20exploration_fixed.ipynb#ch0000001vscode-remote?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_max_sample_idx\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20exploration_fixed.ipynb#ch0000001vscode-remote?line=23'>24</a>\u001b[0m     Post_sample_from_bin_widget\u001b[39m.\u001b[39mmax \u001b[39m=\u001b[39m Nr_histogram_bins_widget\u001b[39m.\u001b[39mvalue\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20exploration_fixed.ipynb#ch0000001vscode-remote?line=26'>27</a>\u001b[0m \u001b[39m@interact\u001b[39m(Show_param_explanation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20exploration_fixed.ipynb#ch0000001vscode-remote?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_param_explanation\u001b[39m(Show_param_explanation):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20exploration_fixed.ipynb#ch0000001vscode-remote?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m Show_param_explanation:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20exploration_fixed.ipynb#ch0000001vscode-remote?line=29'>30</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mParameter explantion:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interact' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ipywidgets as widgets\n",
    "default_hist_bins = 50\n",
    "max_samples = 20\n",
    "\n",
    " \n",
    "f = open('../../feature_explanation.json')\n",
    "data = json.load(f)\n",
    "data['writing_sty_\"_count'] = data.pop(\"writing_sty_'_count\") #quaotation mark hack\n",
    "\n",
    "Post_sample_from_bin_widget = widgets.IntSlider(\n",
    "    min=0, max=default_hist_bins-1, step=1, value=5,\n",
    "    style={'description_width': 'initial'}, layout = widgets.Layout(width='500px'))\n",
    "\n",
    "Nr_histogram_bins_widget = widgets.IntSlider(\n",
    "    min=10, max=500, step=10, value=default_hist_bins, \n",
    "    style={'description_width': 'initial'}, layout = widgets.Layout(width='500px'))\n",
    "\n",
    "Nr_samples_to_get_widget = widgets.IntSlider(\n",
    "    min=1, max=max_samples, step=1, value=1, \n",
    "    style={'description_width': 'initial'}, layout = widgets.Layout(width='500px'))\n",
    "\n",
    "def update_max_sample_idx(*args):\n",
    "    Post_sample_from_bin_widget.max = Nr_histogram_bins_widget.value-1\n",
    "\n",
    "    \n",
    "@interact(Show_param_explanation=True, )\n",
    "def show_param_explanation(Show_param_explanation):\n",
    "    if Show_param_explanation:\n",
    "        print(\"Parameter explantion:\")\n",
    "        print(\"    feature_to_analyse: Which feature we want to get a post_text sample from\")\n",
    "        print(\"    Amount_histo_bins: How many bins the histogram should use. The more bins the more fine grained.\")\n",
    "        print(\"    Bin_index_to_sample: From which histogram bin we want to get a post sample.\")\n",
    "        print(\"    Nr_samples_to_get: How many post samples we want to get. Max is 20.\")\n",
    "        print(\"    Reproducible_random_state: If we always want to get completely random samples within a bin or if they should be reproducible.\")\n",
    "    else:\n",
    "        print(\"Parameter explantion hidden\")\n",
    "\n",
    "def visualiser(feature_to_analyse, Amount_histo_bins, Bin_index_to_sample, Nr_samples_to_get, Reproducible_random_state):\n",
    "    global df_text\n",
    "    p = df[feature_to_analyse].plot(kind='hist', bins=Amount_histo_bins, color='blue')\n",
    "    p.patches[Bin_index_to_sample].set_color('orange')\n",
    "    plt.show()\n",
    "    \n",
    "    truncated = feature_to_analyse.replace(\"_norm\", \"\")\n",
    "    truncated = truncated.replace(\"_abs\",\"\")\n",
    "    \n",
    "    print(\"Feature Explanation:\")\n",
    "    if \"liwc_\" in feature_to_analyse:\n",
    "        print(f\"  {feature_to_analyse} = See https://drive.google.com/file/d/1EHrlt6KcL3jZ5gFAA1vjKd5GdXKLRWmk/view?usp=sharing for a feature explanation\")\n",
    "    elif \"foundations_\" in feature_to_analyse:\n",
    "        print(f\"  {feature_to_analyse} = See https://moralfoundations.org/other-materials/ for a feature explanation\")\n",
    "    elif truncated in data:\n",
    "        print(f\"  {feature_to_analyse} = {data[truncated]}\")\n",
    "    else:\n",
    "        print(f\"  {feature_to_analyse} = Feature explanation not found\")\n",
    "              \n",
    "    min_v = df[feature_to_analyse].min()\n",
    "    max_v = df[feature_to_analyse].max()\n",
    "    \n",
    "    bin_mins = np.linspace(min_v, max_v, num=Amount_histo_bins,endpoint=False)\n",
    "    sample_bin_min = bin_mins[Bin_index_to_sample]\n",
    "    sample_bin_max = max_v if Bin_index_to_sample+1 >= Amount_histo_bins else bin_mins[Bin_index_to_sample+1]\n",
    "    \n",
    "    df_sample = df.loc[((sample_bin_min<=df[feature_to_analyse]) & (df[feature_to_analyse]<=sample_bin_max))]\n",
    "    nr_samples = len(df_sample)\n",
    "    print(f\"   Inspecting values from {sample_bin_min} to {sample_bin_max}. Bucket has {nr_samples} samples\")\n",
    "    \n",
    "    \n",
    "    if nr_samples>=Nr_samples_to_get:\n",
    "        Nr_samples_to_get_widget.max = min(nr_samples, max_samples)\n",
    "        smpl = df_sample['post_id'].sample(n=Nr_samples_to_get, random_state=42 if Reproducible_random_state else None)\n",
    "        df_text_tmp = df_text.loc[df_text[\"post_id\"].isin(smpl)]\n",
    "        \n",
    "        for i in range(len(df_text_tmp)):\n",
    "            post_id = df_text_tmp.iloc[i][\"post_id\"]\n",
    "            post_text = df_text_tmp.iloc[i][\"post_text\"]\n",
    "            print(f\"\\nPOST ID: {post_id}\")\n",
    "            print(f\"POST TEXT:\\n{post_text}\")\n",
    "            print(\"-----------------------\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Not enough samples in this bucket. Wanted {Nr_samples_to_get}, but there are only {nr_samples}\") #this should never happen\n",
    "        \n",
    "Nr_histogram_bins_widget.observe(update_max_sample_idx, 'value')    \n",
    "\n",
    "interact(visualiser, feature_to_analyse=ranking, \n",
    "         Amount_histo_bins=Nr_histogram_bins_widget,\n",
    "         Bin_index_to_sample=Post_sample_from_bin_widget, \n",
    "         Nr_samples_to_get=Nr_samples_to_get_widget,\n",
    "         Reproducible_random_state=False);\n",
    "                                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature explanations\n",
    "# Title features\n",
    "When using the **standalone** dataset all feautres that start with \"title_\" are generated by ONLY looking at the post title. \n",
    "All these features exist also without the \"title_\" prefix which means they were generated by ONLY looking at the post text.\n",
    "\n",
    "When using the **prepend** dataset we never have the \"title_\" prefix since in this case we always prepend the post title to the post text. \n",
    "\n",
    "# LIWC feature\n",
    "All features containg \"liwc_\" were generated by the LIWC software. See [here](https://drive.google.com/file/d/1EHrlt6KcL3jZ5gFAA1vjKd5GdXKLRWmk/view?usp=sharing) for a list of all LIWC features and their explanations.\n",
    "\n",
    "# Moral Foundations features\n",
    "All features containg \"foundations_\" were generated by using the moral foundations dictionary found [here](https://moralfoundations.org/other-materials/) in the LIWC software. It contains 11 features (plus some features that are always automaticall appended by LIWC). Namely, HarmVirtue, HarmVice, FairnessVirtue, FairnessVice, IngroupVirtue, IngroupVice, AuthorityVirtue, AuthorityVice, PurityVirtue, PurityVice, MoralityGeneral,\n",
    "\n",
    "# Speaker features\n",
    "All features containing \"speaker_\":\n",
    "'speaker_author_age' = Age of the speaker/poster. Extracted by looking for expressions like \"My (25, F) boyfirend (33, M) went to get...\"\n",
    "\n",
    "'speaker_author_gender' = Gender of the speaker/poster. Extracted by looking for expressions like \"My (25, F) boyfirend (33, M) went to get...\"\n",
    "\n",
    "'speaker_account_age' = Reddit account age of the poster in days.\n",
    "\n",
    "'speaker_account_comment_karma' = Reddit account comment karma. See [here](https://www.reddit.com/r/NoStupidQuestions/comments/2idfhk/what_is_link_karma/)\n",
    "\n",
    "'speaker_account_link_karma' = Reddit account link karma. See [here](https://www.reddit.com/r/NoStupidQuestions/comments/2idfhk/what_is_link_karma/)\n",
    "\n",
    "# Topic number:\n",
    "Which topic number was assigned to the post. Value is nominal and does not have any meaning excpet for the -1 value which indicates that it was not assigned a topic, because it did not fit well enough into one. \n",
    "\n",
    "# Post features\n",
    "All features containing \"post_\": \n",
    "'post_id' = id of the post as a string.\n",
    "\n",
    "'post_num_comments' = Number of comments this post got.\n",
    "\n",
    "'post_score' = Score of a post. \n",
    "\n",
    "'post_ratio' = Upvote ratio of a post\n",
    "\n",
    "'post_ups' = Amount of post upvotes (approximated using post_score and post_ratio)\n",
    "\n",
    "'post_downs' = Amount of post upvotes (approximated using post_score and post_ratio)\n",
    "\n",
    "# Reaction features\n",
    "All features containing \"reactions_\". Generally means features that describe how people on reddit reacted to the post (excluding up/down votes).\n",
    "\n",
    "'reactions_is_angel' = Whether the post was crossposted to r/AmITheAngel, which would indicate that the poster is acting morally. 1=True, 0=False\n",
    "\n",
    "'reactions_is_devil' = Whether the post was crossposted to r/AmITheDevil, which would indicate that the poster is acting morally. 1=True, 0=False\n",
    "\n",
    "'reactions_YTA' = Amount of comments that wrote YTA.\n",
    "\n",
    "'reactions_NTA' = Amount of comments that wrote NTA.\n",
    "\n",
    "\n",
    "'reactions_INFO' = Amount of comments that wrote INFO.\n",
    "\n",
    "'reactions_ESH' = Amount of comments that wrote ESH.\n",
    "\n",
    "'reactions_NAH' = Amount of comments that wrote NAH.\n",
    "\n",
    "'reactions_weighted_YTA' = Amount of comments that wrote YTA multiplied by the comment score.\n",
    "\n",
    "'reactions_weighted_NTA' = Amount of comments that wrote NTA multiplied by the comment score.\n",
    "\n",
    "'reactions_weighted_INFO' = Amount of comments that wrote INFO multiplied by the comment score.\n",
    "\n",
    "'reactions_weighted_ESH' = Amount of comments that wrote ESH multiplied by the comment score.\n",
    "\n",
    "'reactions_weighted_NAH' = Amount of comments that wrote NAH multiplied by the comment score.\n",
    "\n",
    "\n",
    "# Writing Style features\n",
    "## Normalised and absolute values\n",
    "Normalised values are marked with the \"norm\" suffix wherease absolute values have the \"abs\" suffix. (LIWC values are always normalised between 0-100. Ours are normalised between 0-1)\n",
    "\n",
    "## Symbol counting\n",
    "'writing_sty_!_count' = How often the character ! occures in the post.\n",
    "\n",
    "'writing_sty_\"_count' = How often the character \" occures in the post.\n",
    "\n",
    "'writing_sty_?_count' = How often the character ? occures in the post.\n",
    "\n",
    "## Would I be the asshole (hypothetical)\n",
    "'writing_sty_is_wibta' = Whether the post was a hypothetical and posted with the text \"would I be the asshole\". Extracted with string matching.\n",
    "\n",
    "## Emotions contained within a post.\n",
    "Which emotions + negativity & postitivey were used in the post. Uses the EmoLex dictionary from [here](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). Thus uses string matching like LIWC. Is divided by the number of words in the text if normalised.\n",
    "\n",
    "\n",
    "'writing_sty_negative' = words associated with negative emotions\n",
    "\n",
    "'writing_sty_trust' = words associated with the emotion trust\n",
    "\n",
    "'writing_sty_joy' = words associated with the emotion joy\n",
    "\n",
    "'writing_sty_positive' = words associated with positive emotions\n",
    "\n",
    "'writing_sty_fear' = words associated with the emotion fear\n",
    "\n",
    "'writing_sty_anger' = words associated with the emotion anger\n",
    "\n",
    "'writing_sty_disgust' = words associated with the emotion disgust\n",
    "\n",
    "'writing_sty_surprise' = words associated with the emotion surprise\n",
    "\n",
    "'writing_sty_sadness' = words associated with the emotion sadness\n",
    "\n",
    "'writing_sty_anticipation' = words associated with the emotion anticipation\n",
    "\n",
    "## AITA location\n",
    "Where the question Am I the asshole appears within the text as a percentage of the text. I.e. at the begging of the text 0, at then end 1.\n",
    "Also matches WIBTA and a few different spellings. \n",
    "\n",
    "\n",
    "'writing_sty_aita_count' = How often the question AITA/WIBTA appears in the post\n",
    "\n",
    "'writing_sty_aita_avg_location' = The average location of AITA/WIBTA [0,1]. Is -1 if it never appears.\n",
    "\n",
    "'writing_sty_aita_fst_location' = The fist location of AITA/WIBTA [0,1]. Is -1 if it never appears.\n",
    "\n",
    "'writing_sty_aita_lst_location' = The last location of AITA/WIBTA [0,1]. Is -1 if it never appears.\n",
    "\n",
    "## Profanity\n",
    "How many profane words appear in the text. Is divided by the number of words in the text if normalised.\n",
    "\n",
    "'writing_sty_profanity' = How many profane words there are \n",
    "\n",
    "## Tense\n",
    "In what tense the text is written. We only distinguish past, present and future. Divided by number of sentences if normalised.\n",
    "ML based.\n",
    "\n",
    "'writing_sty_past' = Amount of past tense.\n",
    "\n",
    "'writing_sty_present' = Amount of present tense.\n",
    "\n",
    "'writing_sty_future' = Amount of future tense.\n",
    "\n",
    "## Active & Passive voice\n",
    "In what voice the text is written. Divided by number of sentences if normalised. ML based.\n",
    "\n",
    "'writing_sty_active' = Amount of active voice.\n",
    "\n",
    "'writing_sty_passive' = Amount of passive voice. \n",
    "\n",
    "## Postivity & Subjectivity\n",
    "How positive/Negative a sentence was written + how subjective/objective it was. ML based. \n",
    "\n",
    "'writing_sty_sent_polarity' = How positive/negative the post is. -1 = very negative, 0 = neutral, 1 = very positive\n",
    "\n",
    "'writing_sty_sent_subjectivity' = How subjective/objective the post is. 0 = very objective, 1 = very subjective\n",
    "\n",
    "## Focus Pronoun\n",
    "Check various types of pronouns and count how often they appeared as the subject or object. Also counts possessive pronouns.\n",
    "Divided by number of words in the text if normalised. ML based. \n",
    "\n",
    "'writing_sty_focus_i_subj' = How often pronoun 1. person singular pronoun appeared as the subject.\n",
    "\n",
    "'writing_sty_focus_you_sg_subj' = How often pronoun 2. person singular pronoun appeared as the subject.\n",
    "\n",
    "'writing_sty_focus_he_subj' = How often pronoun 3. person singular pronoun appeared as the subject.\n",
    "\n",
    "'writing_sty_focus_we_subj' = How often pronoun 1. person plural pronoun appeared as the subject.\n",
    "\n",
    "'writing_sty_focus_you_pl_subj' = How often pronoun 2. person plural pronoun appeared as the subject.\n",
    "\n",
    "'writing_sty_focus_they_subj' = How often pronoun 3. person plural pronoun appeared as the subject.\n",
    "\n",
    "'writing_sty_focus_i_obj'= How often pronoun 1. person singular pronoun appeared as the object.\n",
    "\n",
    "'writing_sty_focus_you_sg_obj'= How often pronoun 2. person singular pronoun appeared as the object.\n",
    "\n",
    "'writing_sty_focus_he_obj'= How often pronoun 3. person singular pronoun appeared as the object.\n",
    "\n",
    "'writing_sty_focus_we_obj'= How often pronoun 1. person plural pronoun appeared as the object.\n",
    "\n",
    "'writing_sty_focus_you_pl_obj'= How often pronoun 2. person plural pronoun appeared as the object.\n",
    "\n",
    "'writing_sty_focus_they_obj'= How often pronoun 3. person plural pronoun appeared as the object.\n",
    "\n",
    "'writing_sty_focus_i_poss'= How often 1. person singular possessive pronoun appeared.\n",
    "\n",
    "'writing_sty_focus_you_sg_poss'= How often 2. person singular possessive pronoun appeared.\n",
    "\n",
    "'writing_sty_focus_he_poss'= How often 3. person singular possessive pronoun appeared.\n",
    "\n",
    "'writing_sty_focus_we_poss'= How often 1. person plural possessive pronoun appeared.\n",
    "\n",
    "'writing_sty_focus_you_pl_poss'= How often 2. person plural possessive pronoun appeared.\n",
    "\n",
    "'writing_sty_focus_they_poss'= How often 3. person plural possessive pronoun appeared.\n",
    "\n",
    "\n",
    "\n",
    "## Self vs other emotions\n",
    "Checks the emotions that are either about the self or other people involved. Self focused sentences are defined as sentences where a 1st pronoun is the subject of a sentence. Other focused sentences are sentences with any other pronoun as subject. Sentences w.o. a pronoun as a subject are ignored (I think). ML based, Dictionary based for emotions.\n",
    "\n",
    "\n",
    "'writing_sty_self_fear_norm' = How often the emotion fear is in sentences about the self\n",
    "\n",
    "'writing_sty_self_anger_norm' = How often the emotion anger is in sentences about the self\n",
    "\n",
    "'writing_sty_self_trust_norm' = How often the emotion trust is in sentences about the self\n",
    "\n",
    "'writing_sty_self_surprise_norm' = How often the emotion surprise is in sentences about the self\n",
    "\n",
    "'writing_sty_self_sadness_norm' = How often the emotion sadness is in sentences about the self\n",
    "\n",
    "'writing_sty_self_disgust_norm' = How often the emotion disgust is in sentences about the self\n",
    "\n",
    "'writing_sty_self_joy_norm' = How often the emotion joy is in sentences about the self\n",
    "\n",
    "'writing_sty_self_anticipation_norm' = How often the emotion anticipation is in sentences about the self\n",
    "\n",
    "'writing_sty_self_positive_norm' = How positive sentences about the self are\n",
    "\n",
    "'writing_sty_self_negative_norm' = How negative sentences about the self are\n",
    "\n",
    "'writing_sty_other_fear_norm' = How often the emotion fear is in sentences about others\n",
    "\n",
    "'writing_sty_other_anger_norm' = How often the emotion anger is in sentences about others\n",
    "\n",
    "'writing_sty_other_trust_norm' = How often the emotion trust is in sentences about others\n",
    "\n",
    "'writing_sty_other_surprise_norm' = How often the emotion surprise is in sentences about others\n",
    "\n",
    "'writing_sty_other_sadness_norm' = How often the emotion sadness is in sentences about others\n",
    "\n",
    "'writing_sty_other_disgust_norm' = How often the emotion disgust is in sentences about others\n",
    "\n",
    "'writing_sty_other_joy_norm' = How often the emotion joy is in sentences about others\n",
    "\n",
    "'writing_sty_other_anticipation_norm' = How often the emotion anticipation is in sentences about others\n",
    "\n",
    "'writing_sty_other_positive_norm' = How positive sentences about others are\n",
    "\n",
    "'writing_sty_other_negative_norm' = How negative sentences about others are\n",
    "\n",
    "## Self vs other profanity\n",
    "Checks the profanity amount in sentences either about the self or other people involved. Self focused sentences are defined as sentences where a 1st pronoun is the subject of a sentence. Other focused sentences are sentences with any other pronoun as subject. Sentences w.o. a pronoun as a subject are ignored (I think).ML based, Dictionary based for profanity.\n",
    "\n",
    "\n",
    "'writing_sty_self_prof' = Profanity in sentences about the self\n",
    "\n",
    "'writing_sty_other_prof' = Profanity in sentences about others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "tages": [
   "hide-input"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

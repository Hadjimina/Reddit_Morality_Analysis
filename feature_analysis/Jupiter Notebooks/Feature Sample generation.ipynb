{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(618203, 10)\n",
      "0.5 0.5\n",
      "(30857, 15)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def opposite_jdgmt(judg):\n",
    "    if \"NTA\" in judg:\n",
    "        rtn = judg.replace(\"NTA\", \"YTA\")\n",
    "    elif \"NAH\" in judg:\n",
    "        rtn = judg.replace(\"NAH\", \"ESH\")\n",
    "    elif \"YTA\" in judg:\n",
    "        rtn = judg.replace(\"YTA\", \"NTA\")\n",
    "    elif \"ESH\" in judg:\n",
    "        rtn = judg.replace(\"ESH\", \"NAH\")\n",
    "    elif \"INFO\" in judg:\n",
    "        rtn = judg\n",
    "\n",
    "    return rtn+\"_neg_vals\"\n",
    "\n",
    "# mapping is either \"clip\", meaning negative votes are just set to 0, or \"oppossite\", meaning we use the mapping table in \"opposite_jdgmt\"\n",
    "def map_negative_values(df, acros, mapping=\"clip\"):\n",
    "\n",
    "    if mapping == \"opposite\" or mapping == \"map\":\n",
    "        #print(\"Map = opposite\")\n",
    "        for k in acros.keys():\n",
    "            acr = acros[k]\n",
    "\n",
    "            if k == \"info\":\n",
    "                continue\n",
    "\n",
    "            # create temporary columns containing zeros and only negative votes for each vote type (except info)\n",
    "            df[acr+\"_neg_vals\"] = 0\n",
    "            df.loc[df[acr] < 0, acr+\"_neg_vals\"] = df[acr]*-1\n",
    "            df.loc[df[acr] < 0, acr] = 0\n",
    "\n",
    "        for k in acros.keys():\n",
    "            if k == \"info\":\n",
    "                continue\n",
    "            acr = acros[k]\n",
    "            # set negative values to 0 & add opposite judgement vote\n",
    "            df[acr] = df[acr] + df[opposite_jdgmt(acr)]\n",
    "\n",
    "    elif mapping == \"clip\":\n",
    "        #print(\"Map = clip\")\n",
    "        for k in acros.keys():\n",
    "            acr = acros[k]\n",
    "            df.loc[df[acr] < 0, acr] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_correct_posts(post_requirements, df, doFilter=True):\n",
    "    params = {\n",
    "        \"predict\": \"ratio\",\n",
    "        \"weighted\": True,\n",
    "        \"mapping\": \"map\"\n",
    "    }\n",
    "\n",
    "    params[\"ratio\"] = 0.5 if params[\"predict\"] == \"ratio\" else 0.3\n",
    "    \n",
    "    ratio = params[\"ratio\"]\n",
    "    print(1-ratio,ratio)\n",
    "    keys = [\"info\", \"yta\", \"nah\", \"esh\", \"nta\"]\n",
    "    weight = \"weighted_\" if params[\"weighted\"] else \"\"\n",
    "    values = [\"reactions_\"+weight+k.upper() for k in keys]\n",
    "    acros = dict(zip(keys, values))\n",
    "\n",
    "    df = map_negative_values(df, acros, mapping=params[\"mapping\"])\n",
    "    if params[\"predict\"] == \"ratio\":\n",
    "        tmp = df[acros[\"yta\"]] + df[acros[\"nah\"]] + \\\n",
    "            df[acros[\"esh\"]]+df[acros[\"nta\"]]\n",
    "        tmp = tmp[tmp != 0]\n",
    "        tmp = (df[acros[\"yta\"]]+df[acros[\"esh\"]])/tmp\n",
    "        df[\"Y\"] = tmp\n",
    "\n",
    "        #WHY DOES THIS REMOVE POSTS?\n",
    "        if doFilter:\n",
    "            df = df.loc[(1-ratio <= df[\"Y\"]) | (df[\"Y\"] <= ratio)]\n",
    "    elif params[\"predict\"] == \"class\":\n",
    "        df[\"YTA_ratio\"] = df[acros[\"yta\"]] / \\\n",
    "            (df[acros[\"info\"]] + df[acros[\"yta\"]] +\n",
    "                df[acros[\"nah\"]]+df[acros[\"esh\"]]+df[acros[\"nta\"]])\n",
    "\n",
    "        # drop all rows where the majority is not YTA or NTA\n",
    "        df = df.loc[((df[acros[\"yta\"]] > df[acros[\"info\"]]) & (df[acros[\"yta\"]] > df[acros[\"nah\"]]) & (df[acros[\"yta\"]] > df[acros[\"esh\"]])) | (\n",
    "            (df[acros[\"nta\"]] > df[acros[\"info\"]]) & (df[acros[\"nta\"]] > df[acros[\"nah\"]]) & (df[\"reactions_weighted_NTA\"] > df[acros[\"esh\"]]))]\n",
    "\n",
    "        # drop all rows that are not \"extreme\" enough\n",
    "        df = df.loc[(1-ratio <= df[\"YTA_ratio\"]) | (df[\"YTA_ratio\"] <= ratio)]\n",
    "        print(\"CHECK REGARDING FILTERING\")\n",
    "\n",
    "    if doFilter:\n",
    "        for k, v in post_requirements.items():\n",
    "            df = df.loc[(df[k] >= v), :]\n",
    "\n",
    "    #df = df[df.columns.intersection(features_to_analyse+[\"post_id\"])]\n",
    "    return df\n",
    "\n",
    "# [\"liwc_female\", \"liwc_discrep\", \"liwc_home\", \"writing_sty_focus_i_poss_norm\"]\n",
    "features_to_analyse = [\"writing_sty_focus_i_poss_norm\", ]\n",
    "reactions_to_keep = [\"reactions_weighted_YTA\", \"reactions_weighted_NTA\",\n",
    "                     \"reactions_weighted_ESH\", \"reactions_weighted_NAH\", \"reactions_weighted_INFO\"]\n",
    "\n",
    "post_requirements = {  # requirement: key >= value in post\n",
    "        \"post_num_comments\": 10,\n",
    "        \"post_score\": 10,\n",
    "        \"post_ratio\": 0.7,\n",
    "    }\n",
    "\n",
    "#df = pd.read_csv(\"../datasets/prepend_done.csv\")\n",
    "df = pd.read_csv(\"../datasets/prepend_done.csv\",\n",
    "                 usecols=features_to_analyse+reactions_to_keep+list(post_requirements.keys())+[\"post_id\"])\n",
    "\n",
    "print(df.shape)\n",
    "df = get_correct_posts(post_requirements, df, doFilter=True)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "df_txt = pd.read_csv(\"../datasets/id_to_text.csv\")\n",
    "#lst = list(filter(lambda x: \"sty\" in x, list(df.columns)))\n",
    "# print(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speaker_author_age', 'speaker_author_gender', 'post_id', 'post_num_comments', 'reactions_weighted_YTA', 'reactions_weighted_NTA', 'reactions_weighted_INFO', 'reactions_weighted_ESH', 'reactions_weighted_NAH', 'reactions_weighted_YTA_neg_vals', 'reactions_weighted_NTA_neg_vals', 'reactions_weighted_INFO_neg_vals', 'reactions_weighted_ESH_neg_vals', 'reactions_weighted_NAH_neg_vals', 'writing_sty_!_count', 'writing_sty_\"_count', 'writing_sty_?_count', 'writing_sty_negative_abs', 'writing_sty_trust_abs', 'writing_sty_joy_abs', 'writing_sty_positive_abs', 'writing_sty_fear_abs', 'writing_sty_anger_abs', 'writing_sty_disgust_abs', 'writing_sty_surprise_abs', 'writing_sty_sadness_abs', 'writing_sty_anticipation_abs', 'writing_sty_negative_norm', 'writing_sty_trust_norm', 'writing_sty_joy_norm', 'writing_sty_positive_norm', 'writing_sty_fear_norm', 'writing_sty_anger_norm', 'writing_sty_disgust_norm', 'writing_sty_surprise_norm', 'writing_sty_sadness_norm', 'writing_sty_anticipation_norm', 'writing_sty_aita_count', 'writing_sty_aita_avg_location', 'writing_sty_aita_fst_location', 'writing_sty_aita_lst_location', 'writing_sty_profanity_abs', 'writing_sty_profanity_norm', 'writing_sty_is_wibta', 'writing_sty_past_abs', 'writing_sty_present_abs', 'writing_sty_future_abs', 'writing_sty_past_norm', 'writing_sty_present_norm', 'writing_sty_future_norm', 'writing_sty_active_abs', 'writing_sty_passive_abs', 'writing_sty_active_norm', 'writing_sty_passive_norm', 'writing_sty_sent_polarity', 'writing_sty_sent_subjectivity', 'writing_sty_focus_i_subj_abs', 'writing_sty_focus_you_sg_subj_abs', 'writing_sty_focus_he_subj_abs', 'writing_sty_focus_we_subj_abs', 'writing_sty_focus_you_pl_subj_abs', 'writing_sty_focus_they_subj_abs', 'writing_sty_focus_i_obj_abs', 'writing_sty_focus_you_sg_obj_abs', 'writing_sty_focus_he_obj_abs', 'writing_sty_focus_we_obj_abs', 'writing_sty_focus_you_pl_obj_abs', 'writing_sty_focus_they_obj_abs', 'writing_sty_focus_i_poss_abs', 'writing_sty_focus_you_sg_poss_abs', 'writing_sty_focus_he_poss_abs', 'writing_sty_focus_we_poss_abs', 'writing_sty_focus_you_pl_poss_abs', 'writing_sty_focus_they_poss_abs', 'writing_sty_focus_i_subj_norm', 'writing_sty_focus_you_sg_subj_norm', 'writing_sty_focus_he_subj_norm', 'writing_sty_focus_we_subj_norm', 'writing_sty_focus_you_pl_subj_norm', 'writing_sty_focus_they_subj_norm', 'writing_sty_focus_i_obj_norm', 'writing_sty_focus_you_sg_obj_norm', 'writing_sty_focus_he_obj_norm', 'writing_sty_focus_we_obj_norm', 'writing_sty_focus_you_pl_obj_norm', 'writing_sty_focus_they_obj_norm', 'writing_sty_focus_i_poss_norm', 'writing_sty_focus_you_sg_poss_norm', 'writing_sty_focus_he_poss_norm', 'writing_sty_focus_we_poss_norm', 'writing_sty_focus_you_pl_poss_norm', 'writing_sty_focus_they_poss_norm', 'writing_sty_self_fear_norm', 'writing_sty_self_anger_norm', 'writing_sty_self_trust_norm', 'writing_sty_self_surprise_norm', 'writing_sty_self_sadness_norm', 'writing_sty_self_disgust_norm', 'writing_sty_self_joy_norm', 'writing_sty_self_anticipation_norm', 'writing_sty_self_positive_norm', 'writing_sty_self_negative_norm', 'writing_sty_other_fear_norm', 'writing_sty_other_anger_norm', 'writing_sty_other_trust_norm', 'writing_sty_other_surprise_norm', 'writing_sty_other_sadness_norm', 'writing_sty_other_disgust_norm', 'writing_sty_other_joy_norm', 'writing_sty_other_anticipation_norm', 'writing_sty_other_positive_norm', 'writing_sty_other_negative_norm', 'writing_sty_self_prof', 'writing_sty_other_prof', 'foundations_WC', 'foundations_WPS', 'foundations_Sixltr', 'foundations_Dic', 'foundations_01                    HarmVirtue', 'foundations_02                    HarmVice', 'foundations_03                    FairnessVirtue', 'foundations_04                    FairnessVice', 'foundations_05                    IngroupVirtue', 'foundations_06                    IngroupVice', 'foundations_07                    AuthorityVirtue', 'foundations_08                    AuthorityVice', 'foundations_09                    PurityVirtue', 'foundations_10                    PurityVice', 'foundations_11                    MoralityGeneral', 'foundations_AllPunc', 'foundations_Period', 'foundations_Comma', 'foundations_Colon', 'foundations_SemiC', 'foundations_QMark', 'foundations_Exclam', 'foundations_Dash', 'foundations_Quote', 'foundations_Apostro', 'foundations_Parenth', 'foundations_OtherP', 'liwc_WC', 'liwc_Analytic', 'liwc_Clout', 'liwc_Authentic', 'liwc_Tone', 'liwc_WPS', 'liwc_Sixltr', 'liwc_Dic', 'liwc_function', 'liwc_pronoun', 'liwc_ppron', 'liwc_i', 'liwc_we', 'liwc_you', 'liwc_shehe', 'liwc_they', 'liwc_ipron', 'liwc_article', 'liwc_prep', 'liwc_auxverb', 'liwc_adverb', 'liwc_conj', 'liwc_negate', 'liwc_verb', 'liwc_adj', 'liwc_compare', 'liwc_interrog', 'liwc_number', 'liwc_quant', 'liwc_affect', 'liwc_posemo', 'liwc_negemo', 'liwc_anx', 'liwc_anger', 'liwc_sad', 'liwc_social', 'liwc_family', 'liwc_friend', 'liwc_female', 'liwc_male', 'liwc_cogproc', 'liwc_insight', 'liwc_cause', 'liwc_discrep', 'liwc_tentat', 'liwc_certain', 'liwc_differ', 'liwc_percept', 'liwc_see', 'liwc_hear', 'liwc_feel', 'liwc_bio', 'liwc_body', 'liwc_health', 'liwc_sexual', 'liwc_ingest', 'liwc_drives', 'liwc_affiliation', 'liwc_achieve', 'liwc_power', 'liwc_reward', 'liwc_risk', 'liwc_focuspast', 'liwc_focuspresent', 'liwc_focusfuture', 'liwc_relativ', 'liwc_motion', 'liwc_space', 'liwc_time', 'liwc_work', 'liwc_leisure', 'liwc_home', 'liwc_money', 'liwc_relig', 'liwc_death', 'liwc_informal', 'liwc_swear', 'liwc_netspeak', 'liwc_assent', 'liwc_nonflu', 'liwc_filler', 'liwc_AllPunc', 'liwc_Period', 'liwc_Comma', 'liwc_Colon', 'liwc_SemiC', 'liwc_QMark', 'liwc_Exclam', 'liwc_Dash', 'liwc_Quote', 'liwc_Apostro', 'liwc_Parenth', 'liwc_OtherP', 'speaker_account_age', 'speaker_account_comment_karma', 'speaker_account_link_karma', 'topic_nr', 'post_score', 'post_ups', 'post_downs', 'post_ratio', 'reactions_is_angel', 'reactions_is_devil', 'reactions_YTA', 'reactions_NTA', 'reactions_INFO', 'reactions_ESH', 'reactions_NAH', 'Y']\n"
     ]
    }
   ],
   "source": [
    "print(list(df.columns))\n",
    "#df.to_csv(\"/mnt/c/Users/Philipp/Desktop/dataset_prepend_y_filtered.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acros Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter Notebooks/Feature Sample generation.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20Sample%20generation.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m bin_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\u001b[39m/\u001b[39m(nr_bins\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20Sample%20generation.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m quantile \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20Sample%20generation.ipynb#ch0000003vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m df\u001b[39m.\u001b[39mlength \u001b[39m>\u001b[39m \u001b[39m380000\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20Sample%20generation.ipynb#ch0000003vscode-remote?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWARNING: we probably want to use the filtered output. Check above to make sure you use doFilter = True. Otherwise we sample posts that our ML model has never seen\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/philipp/Documents/Coding/Reddit_Morality_Analysis/feature_analysis/Jupiter%20Notebooks/Feature%20Sample%20generation.ipynb#ch0000003vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m feat \u001b[39min\u001b[39;00m features_to_analyse:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "nr_bins = 21\n",
    "bin_size = 100/(nr_bins-1)\n",
    "quantile = 100\n",
    "\n",
    "if df.length > 380000:\n",
    "    print(\"WARNING: we probably want to use the filtered output. Check above to make sure you use doFilter = True. Otherwise we sample posts that our ML model has never seen\")\n",
    "\n",
    "for feat in features_to_analyse:\n",
    "\n",
    "    min_v = df[feat].min()\n",
    "    max_v = df[feat].max()\n",
    "    clipped_df = df\n",
    "    print(min_v,  max_v)\n",
    "    print(clipped_df.shape)\n",
    "    #if quantile < 100:\n",
    "    #    print(df.shape)\n",
    "    #    \n",
    "    #    clipped_df = df[(np.percentile(df[feat], 100-quantile) < df[feat]) & (\n",
    "    #        df[feat] < np.percentile(df[feat],quantile ))]\n",
    "    #    print(clipped_df.shape)\n",
    "    #min_v = clipped_df[feat].min()\n",
    "    #max_v = clipped_df[feat].max()\n",
    "    #print(min_v, max_v)\n",
    "    bin_mins = np.linspace(min_v, max_v, num=nr_bins, endpoint=True)\n",
    "    decinels = np.linspace(0, 100, num=nr_bins, endpoint=True)\n",
    "\n",
    "    for i in range(len(bin_mins)-1):\n",
    "        #print(i,bin_mins[i], bin_mins[i+1] )\n",
    "        df_sample = clipped_df.loc[((bin_mins[i] <= clipped_df[feat]) &\n",
    "                            (clipped_df[feat] <= bin_mins[i+1]))]\n",
    "        if len(df_sample) > 0:\n",
    "            smpl_id = df_sample['post_id'].sample(n=1, random_state=42).iloc[0]\n",
    "            smpl_value = str(df_sample[df_sample[\"post_id\"]\n",
    "                                   == smpl_id][feat].iloc[0])\n",
    "            smpl_text = df_txt.loc[df_txt[\"post_id\"]\n",
    "                                   == smpl_id].iloc[0][\"post_text\"]\n",
    "            smpl_y = df_sample[df_sample[\"post_id\"]\n",
    "                                   == smpl_id][\"Y\"].iloc[0]\n",
    "        else:\n",
    "            print(f\"No posts found for {feat} from {int(100-i*bin_size)}%\")\n",
    "            smpl_id = \"BUCKET EMPTY\"\n",
    "            smpl_value = \"-42\"\n",
    "            smpl_text = \"NA\"\n",
    "\n",
    "        cur = [feat, f\"{int(i*bin_size)}%\",\n",
    "               smpl_value, smpl_y,smpl_id, smpl_text]\n",
    "        data.append(cur)\n",
    "\n",
    "np_data = np.array(data)\n",
    "df_ft_smpl = pd.DataFrame(data=data, columns=[\n",
    "                          \"feature Name\", \"range\", \"feature value\", \"y\", \"post_id\", \"post_text\"])\n",
    "df_ft_smpl.to_excel(\"feature_samples.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Across percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.12676056\n",
      "(30857, 15)\n",
      "Index(['post_id', 'post_num_comments', 'reactions_weighted_YTA',\n",
      "       'reactions_weighted_NTA', 'reactions_weighted_INFO',\n",
      "       'reactions_weighted_ESH', 'reactions_weighted_NAH',\n",
      "       'writing_sty_focus_i_poss_norm', 'post_score', 'post_ratio',\n",
      "       'reactions_weighted_YTA_neg_vals', 'reactions_weighted_NAH_neg_vals',\n",
      "       'reactions_weighted_ESH_neg_vals', 'reactions_weighted_NTA_neg_vals',\n",
      "       'Y'],\n",
      "      dtype='object')\n",
      "Bucket (size 777) = from: perc [0.0-2.5], value [0.0-0.0]\n",
      "Bucket (size 1554) = from: perc [2.5-5.0], value [0.0-0.0048076925]\n",
      "Bucket (size 783) = from: perc [5.0-7.5], value [0.0048076925-0.0064935065]\n",
      "Bucket (size 777) = from: perc [7.5-10.0], value [0.0064935065-0.007867828000000006]\n",
      "Bucket (size 794) = from: perc [10.0-12.5], value [0.007867828000000006-0.009009009]\n",
      "Bucket (size 777) = from: perc [12.5-15.0], value [0.009009009-0.010067114]\n",
      "Bucket (size 779) = from: perc [15.0-17.5], value [0.010067114-0.011079455399999997]\n",
      "Bucket (size 773) = from: perc [17.5-20.0], value [0.011079455399999997-0.012019231]\n",
      "Bucket (size 779) = from: perc [20.0-22.5], value [0.012019231-0.012931035]\n",
      "Bucket (size 802) = from: perc [22.5-25.0], value [0.012931035-0.013793103]\n",
      "Bucket (size 788) = from: perc [25.0-27.5], value [0.013793103-0.014652015]\n",
      "Bucket (size 776) = from: perc [27.5-30.0], value [0.014652015-0.0154639175]\n",
      "Bucket (size 793) = from: perc [30.0-32.5], value [0.0154639175-0.016286645]\n",
      "Bucket (size 788) = from: perc [32.5-35.0], value [0.016286645-0.017142856]\n",
      "Bucket (size 800) = from: perc [35.0-37.5], value [0.017142856-0.017964073]\n",
      "Bucket (size 778) = from: perc [37.5-40.0], value [0.017964073-0.018796992]\n",
      "Bucket (size 776) = from: perc [40.0-42.5], value [0.018796992-0.019676823799999996]\n",
      "Bucket (size 773) = from: perc [42.5-45.0], value [0.019676823799999996-0.020560747]\n",
      "Bucket (size 777) = from: perc [45.0-47.5], value [0.020560747-0.021472393]\n",
      "Bucket (size 775) = from: perc [47.5-50.0], value [0.021472393-0.022375215]\n",
      "Bucket (size 775) = from: perc [50.0-52.5], value [0.022375215-0.023333333]\n",
      "Bucket (size 779) = from: perc [52.5-55.0], value [0.023333333-0.024305556]\n",
      "Bucket (size 782) = from: perc [55.0-57.5], value [0.024305556-0.025316456]\n",
      "Bucket (size 827) = from: perc [57.5-60.0], value [0.025316456-0.02631579]\n",
      "Bucket (size 817) = from: perc [60.0-62.5], value [0.02631579-0.02745098]\n",
      "Bucket (size 826) = from: perc [62.5-65.0], value [0.02745098-0.028571429]\n",
      "Bucket (size 789) = from: perc [65.0-67.5], value [0.028571429-0.029739777]\n",
      "Bucket (size 788) = from: perc [67.5-70.0], value [0.029739777-0.031088082]\n",
      "Bucket (size 773) = from: perc [70.0-72.5], value [0.031088082-0.032388665]\n",
      "Bucket (size 785) = from: perc [72.5-75.0], value [0.032388665-0.03380282]\n",
      "Bucket (size 783) = from: perc [75.0-77.5], value [0.03380282-0.03529412]\n",
      "Bucket (size 782) = from: perc [77.5-80.0], value [0.03529412-0.03691275]\n",
      "Bucket (size 782) = from: perc [80.0-82.5], value [0.03691275-0.03875969]\n",
      "Bucket (size 793) = from: perc [82.5-85.0], value [0.03875969-0.040816326]\n",
      "Bucket (size 788) = from: perc [85.0-87.5], value [0.040816326-0.043243244]\n",
      "Bucket (size 775) = from: perc [87.5-90.0], value [0.043243244-0.04610951]\n",
      "Bucket (size 772) = from: perc [90.0-92.5], value [0.04610951-0.04947732760000002]\n",
      "Bucket (size 778) = from: perc [92.5-95.0], value [0.04947732760000002-0.054054055]\n",
      "Bucket (size 787) = from: perc [95.0-97.5], value [0.054054055-0.06166928479999999]\n",
      "Bucket (size 772) = from: perc [97.5-100.0], value [0.06166928479999999-0.12676056]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "nr_bins = 41\n",
    "bin_size = 100/(nr_bins-1)\n",
    "quantile = 100\n",
    "\n",
    "for feat in features_to_analyse:\n",
    "\n",
    "    min_v = df[feat].min()\n",
    "    max_v = df[feat].max()\n",
    "    clipped_df = df\n",
    "    print(min_v,  max_v)\n",
    "    print(clipped_df.shape)\n",
    "    print(clipped_df.columns)\n",
    "    #if quantile < 100:\n",
    "    #    print(df.shape)\n",
    "    #    \n",
    "    #    clipped_df = df[(np.percentile(df[feat], 100-quantile) < df[feat]) & (\n",
    "    #        df[feat] < np.percentile(df[feat],quantile ))]\n",
    "    #    print(clipped_df.shape)\n",
    "    #min_v = clipped_df[feat].min()\n",
    "    #max_v = clipped_df[feat].max()\n",
    "    #print(min_v, max_v)\n",
    "    #bin_mins = np.linspace(min_v, max_v, num=nr_bins, endpoint=True)\n",
    "    percentiles = np.linspace(0, 100, num=nr_bins, endpoint=True)\n",
    "\n",
    "    for i in range(len(percentiles)-1):\n",
    "        #print(i,bin_mins[i], bin_mins[i+1] )\n",
    "        min_val = np.percentile(clipped_df[feat], percentiles[i])\n",
    "        max_val = np.percentile(clipped_df[feat], percentiles[i+1])\n",
    "        df_sample = clipped_df.loc[((min_val <= clipped_df[feat]) &\n",
    "                            (clipped_df[feat] <= max_val))]\n",
    "        print(f\"Bucket (size {len(df_sample)}) = from: perc [{percentiles[i]}-{percentiles[i+1]}], value [{min_val}-{max_val}]\")\n",
    "        \n",
    "        \n",
    "        if len(df_sample) > 0:\n",
    "            smpl_id = df_sample['post_id'].sample(n=1).iloc[0]\n",
    "            smpl_value = str(df_sample[df_sample[\"post_id\"]\n",
    "                                   == smpl_id][feat].iloc[0])\n",
    "            smpl_text = df_txt.loc[df_txt[\"post_id\"]\n",
    "                                   == smpl_id].iloc[0][\"post_text\"]\n",
    "            smpl_y = df_sample[df_sample[\"post_id\"]\n",
    "                                   == smpl_id][\"Y\"].iloc[0]\n",
    "        else:\n",
    "            print(f\"No posts found for {feat} from {int(100-i*bin_size)}%\")\n",
    "            smpl_id = \"BUCKET EMPTY\"\n",
    "            smpl_value = \"-42\"\n",
    "            smpl_text = \"NA\"\n",
    "\n",
    "        cur = [feat, f\"{int(i*bin_size)}%\",\n",
    "               smpl_value, smpl_y,smpl_id, smpl_text]\n",
    "        data.append(cur)\n",
    "\n",
    "np_data = np.array(data)\n",
    "df_ft_smpl = pd.DataFrame(data=data, columns=[\n",
    "                          \"feature Name\", \"range\", \"feature value\", \"y\", \"post_id\", \"post_text\"])\n",
    "df_ft_smpl.to_excel(\"feature_samples.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

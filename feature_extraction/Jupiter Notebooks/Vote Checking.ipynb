{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from datetime import datetime, date\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import nltk\n",
    "import socket\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import pandas as pd\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "\n",
    "def find_all(a_str, sub):\n",
    "    \"\"\"Find all substring occurences (non overlapping)\n",
    "\n",
    "    Args:\n",
    "        a_str (string): some string\n",
    "        sub (string): some substring we want to find within a_str\n",
    "\n",
    "    Yields:\n",
    "        list: list of indices\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1:\n",
    "            return\n",
    "        yield start\n",
    "        start += len(sub)\n",
    "\n",
    "def dict_to_feature_tuples(dict, suffix=\"\"):\n",
    "    \"\"\" Take a dict at end of a feature function and converts it into the tuple format suitable for the dataframe\n",
    "\n",
    "    Args:\n",
    "        dict: dictionary containing features data\n",
    "        suffix: string we post pend to each feature category\n",
    "\n",
    "    Returns:\n",
    "        tuple_list: list of tuples for the dataframe e.g. [(feature name, value),...]\n",
    "    \"\"\"\n",
    "    tuple_list = []\n",
    "    #all_values_zero = True\n",
    "    for k, v in dict.items():\n",
    "        tpl = ((\"{0}\"+suffix).format(k), v)\n",
    "        # if not np.isclose(v, 0, rtol=1e-05, atol=1e-08, equal_nan=False):\n",
    "        #    all_values_zero = False\n",
    "        tuple_list.append(tpl)\n",
    "\n",
    "    return tuple_list  # if not all_values_zero else []\n",
    "\n",
    "\n",
    "\n",
    "def flatten_list(lst):\n",
    "    \"\"\"Flattens a list to only 1 dimension\n",
    "\n",
    "    Args:\n",
    "        lst (list): Any type of list    \n",
    "\n",
    "    Returns:\n",
    "        flattened list: Flat list of the original list\n",
    "    \"\"\"\n",
    "    return [item for sublist in lst for item in sublist]\n",
    "\n",
    "def get_clean_text(post_text,\n",
    "                   nlp,\n",
    "                   remove_URL=True,\n",
    "                   remove_punctuation=0,\n",
    "                   remove_newline=True,\n",
    "                   merge_whitespaces=True,\n",
    "                   do_lowercaseing=True,\n",
    "                   remove_stopwords=False,\n",
    "                   do_lemmatization=True,\n",
    "                   remove_am=False):\n",
    "    \"\"\"Function to clean text (i.e. remove urls, punctuation, newlines etc) and do lemmatizaion if needed\n",
    "\n",
    "    Args:\n",
    "        post_text (string): full body text of r/AITA posts\n",
    "        nlp (function): nlp function from the spacy object\n",
    "        remove_URL (bool, optional): Whether or not URLs should be removed. Defaults to True.\n",
    "        remove_punctuation (int, optional): Whether or not punctuation should be removed. Defaults to False.\n",
    "        remove_newline (bool, optional): Whether or not newline characters should be removed. Defaults to True.\n",
    "        merge_whitespaces (bool, optional): Whether or not mulitple consecutive whitespace should be merged to one. Defaults to True.\n",
    "        do_lowercaseing (bool, optional): Whether or not text should be lowercased. Defaults to True.\n",
    "        remove_stopwords (bool, optional): Whether or not stopwords from nltk should be removed (includes here, than, myself, which, it....). Defaults to False.\n",
    "        do_lemmatization (bool, optional): Whether or not we should return the lemmatized post. Defaults to True.\n",
    "        remove_am (bool, optional): Whether or not we should remove all \"'m\" and \"am\" in the post. Defaults to False\n",
    "\n",
    "    Returns:\n",
    "        string/spacy doc: Cleaned string or cleaned & lemmatized spacy doc. Spacydoc can be iterated over just like one would a string\n",
    "    \"\"\"\n",
    "\n",
    "    #remove emojis:\n",
    "    post_text = emoji.get_emoji_regexp().sub(u'', post_text)\n",
    "\n",
    "    if remove_am:\n",
    "        post_text = post_text.replace(\"'m\", \" \").replace(\"am\", \" \")\n",
    "\n",
    "    if remove_URL:\n",
    "        post_text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(post_text))\n",
    "\n",
    "    if remove_punctuation == 1:\n",
    "        post_text = post_text.translate(\n",
    "            str.maketrans(' ', ' ', string.punctuation))\n",
    "    elif remove_punctuation == 2:\n",
    "        post_text = post_text.translate(str.maketrans(\n",
    "            string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "    # \\n = newline & \\r = carriage return\n",
    "    if remove_newline:\n",
    "        post_text = post_text.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "    if merge_whitespaces:\n",
    "        post_text = ' '.join(post_text.split())\n",
    "\n",
    "    if do_lowercaseing:\n",
    "        post_text = post_text.lower()\n",
    "\n",
    "    # removes things like [i, me, my, myself, we, our, ours, ...\n",
    "    if remove_stopwords:\n",
    "        post_text = \" \".join(\n",
    "            [word for word in post_text.split() if word not in stopwords.words('english')])\n",
    "\n",
    "    if do_lemmatization:\n",
    "        return nlp(post_text)  # spacy\n",
    "    else:\n",
    "        return post_text\n",
    "\n",
    "def string_matching_arr_append_ah(matches):\n",
    "    \"\"\"When performing string matching we sometimes manualy specific which words to match. Often these include the words \"asshole\".\n",
    "        Often users on AITA, do not write \"asshole\" but write \"ah\" instead. Thus we need to extend the matching list but replace all \"asshole\" occurences with \"ah\"\n",
    "\n",
    "    Args:\n",
    "        matches (list): list of matching strings, these do not include any \"ah\" yet but only \"asshole\"\n",
    "\n",
    "    Returns:\n",
    "        matches_extended: list of matching strings extended to include \"asshole\" and \"ah\n",
    "    \"\"\"\n",
    "\n",
    "    asshole_str = \"asshole\"\n",
    "    ah_str_list = [\"ah\",\"a-hole\", \"a hole\", \"ass hole\", \"a**hole\", \"a** hole\"]\n",
    "\n",
    "    ah_to_post_pend = []\n",
    "    for ah_str in ah_str_list:\n",
    "        for match_str in matches:\n",
    "            if asshole_str in match_str:\n",
    "                ah_to_post_pend += [match_str.replace(asshole_str, ah_str)]\n",
    "\n",
    "    matches_extended = matches + ah_to_post_pend\n",
    "    return matches_extended\n",
    "\n",
    "def get_judgement_labels(df_cmt):\n",
    "    \"\"\"Returns judgement label counts (YTA, NTA, INFO, ESH, NAH)\n",
    "\n",
    "    Args:\n",
    "        post_id (int): Id of the reddit post\n",
    "\n",
    "    Returns:\n",
    "        tuple list (list): e.g. [(\"NTA\",10), (\"YTA\", 20),...]\n",
    "    \"\"\"\n",
    "    JUDGMENT_ACRONYM = [\"YTA\", \"NTA\", \"INFO\", \"ESH\", \"NAH\"]\n",
    "    JUDGEJMENT_DICT = { # everytime asshole appears an expression get added that replaces asshole with \"ah\"\n",
    "        \"NTA\": [\"NTA\", \"not the asshole\", \"not an asshole\", \"don't think you're an asshole\", \"YWNBTA\"],\n",
    "        \"INFO\": [\"INFO\", \"Not enough info\", \"Not enough information\", \"More info\", ],\n",
    "        \"ESH\": [\"ESH\", \"everyone sucks here\", \"everybody sucks here\", \"ETA\", \"everyone's the asshole\"],\n",
    "        \"NAH\": [\"NAH\", \"No Assholes here\", \"No Asshole here\", \"no one sucks here\"],\n",
    "        \"YTA\": [\"YTA\", \"You're the asshole\", \"You are the asshole\", \"You are a little bit the asshole\", \"YWBTA\", \"You are an asshole\", \"You're an asshole\", \"yt absolute a\"],\n",
    "    }\n",
    "    BOT_STRINGS = [\"automod\", \"i am a bot\"]\n",
    "\n",
    "    df_comments = df_cmt\n",
    "    df_comments = df_comments[[\"comment_text\", \"comment_score\"]]\n",
    "\n",
    "    label_counter = JUDGMENT_ACRONYM + [\"weighted_\"+s for s in JUDGMENT_ACRONYM]\n",
    "    label_counter = dict.fromkeys(label_counter,0)\n",
    "    \n",
    "    \n",
    "    for i, comment_row in enumerate(df_comments.itertuples(), 1):\n",
    "        _, comment_body, score = comment_row\n",
    "    \n",
    "        comment_body = get_clean_text(str(comment_body), None, do_lemmatization=False)\n",
    "        comment_body_no_punct = get_clean_text(str(comment_body), None, remove_punctuation=2, do_lemmatization=False)\n",
    "\n",
    "        if any(list(map(lambda x: x.lower() in comment_body, BOT_STRINGS))):\n",
    "            #print(\"___SKIPPED___\")\n",
    "            continue\n",
    "\n",
    "        labels_loc = {}\n",
    "\n",
    "        middle = max(len(comment_body)//2,1)\n",
    "        middle_simple = max(len(comment_body_no_punct.split())//2,1)\n",
    "        \n",
    "        for k in JUDGEJMENT_DICT.keys():\n",
    "            idxes = []              # \"e.g. You are the asshole\"\n",
    "            center_dist = []                 \n",
    "\n",
    "            for x in string_matching_arr_append_ah(JUDGEJMENT_DICT[k]):   \n",
    "                if len(x.split()) > 1: \n",
    "                    idxes = find_all(comment_body, x.lower())\n",
    "                    idxes = list(filter(lambda x: x != -1, idxes))\n",
    "                    center_dist_tmp = list(map(lambda q: (abs(middle-q) / middle), idxes ))\n",
    "                    \n",
    "                else: \n",
    "                    idxes = [i for i,y in enumerate(comment_body_no_punct.split()) if y==x.lower()]\n",
    "                    center_dist_tmp = list(map(lambda q: (abs(middle_simple-q) / middle_simple), idxes ))\n",
    "\n",
    "                # No longer index but distance from center\n",
    "                center_dist_tmp.sort(reverse=True)\n",
    "                center_dist += center_dist_tmp\n",
    "                \n",
    "            # Order by distance\n",
    "            #merged = center_dist + center_dist_simple\n",
    "            center_dist.sort(reverse=True)\n",
    "            labels_loc[k] = center_dist\n",
    "\n",
    "        # Check if more than one vote was detected\n",
    "        nr_votes = len(flatten_list(list(labels_loc.values())))\n",
    "        vote = \"\"\n",
    "        if nr_votes > 1:\n",
    "            # We remove info since this could often cause errors and we are not super interested in it\n",
    "            labels_loc.pop(\"INFO\", None)\n",
    "            max_label = \"\"\n",
    "            max_value = 0\n",
    "            for k in labels_loc.keys():\n",
    "                if len(labels_loc[k]) > 0 and labels_loc[k][0] > max_value:\n",
    "                    max_value = labels_loc[k][0]\n",
    "                    max_label = k\n",
    "        \n",
    "            vote = max_label\n",
    "        else:\n",
    "            # Take first dict entry that contains some value\n",
    "            for k in labels_loc.keys():\n",
    "                if len(labels_loc[k]) > 0:\n",
    "                    vote = k\n",
    "    \n",
    "        #print(labels_loc)\n",
    "        #print(vote)\n",
    "        #print(\"___end___\")\n",
    "        if vote != \"\":\n",
    "            label_counter[vote.upper()] += 1\n",
    "            label_counter[\"weighted_\"+vote.upper()] += int(score)\n",
    "    \n",
    "    tuple_list =  dict_to_feature_tuples(label_counter) \n",
    "\n",
    "    return tuple_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "comments = [[\"Your wife got to enjoy her meal at her own pace while you dealt with the cranky kids; did the bedtime routine alone, and then also cleaned the kitchen afterwards. I‚Äôd be grateful. NTA.\", 1],\n",
    "            [\"NTA. 1-1.5 hours is excessive, especially with small children\", 1],\n",
    "            [\"INFO:How does it take someone one and half hours to eat their food? Nope, nevermind, how does it commonly take an hour?!Is she just talking the whole time? Is she reading her phone? Wth is she doing?NTA since your wife is rude here. If everyone else has finished eating it is polite to speed up. If you maintain your slow eating rate you are just being selfish.EDIT: I concede there are genuine reasons for people to be slow eaters and if they fall into those categories then it is instead reasonable to simple not expect others to wait for you\",1],\n",
    "            [\"It's ok, your marriage probably won't survive, but at least you'll keep your job  ü§∑‚Äç‚ôÇÔ∏èYTA\",1],\n",
    "            [\"You are the asshole. This can‚Äôt be real. If your job depends on you not kicking a deliberate provocateur out of your own wedding, regardless of who that person is, then you need a new job yesterday. As it is, good luck with the annulment, because I don‚Äôt know how or why you expect to stay married after demonstrating you don‚Äôt actually qualify as a vertebrate. Then again, I don‚Äôt know why your wife didn‚Äôt take matters into her own hands and kick these people out herself, so‚Ä¶\",1],\n",
    "            [\"Son: You're not my dad! You're not the grandfather to my children. My real dad was a marine.Stepdad: Okay then. Go to your 'real dad' if you want help.Son: <Surprised pikachu face>...........ESH.You for not talking about your feelings sooner to your son.Your son for not realising how much his comments hurt and relying on your help while simultaneously trying to erase your relationship with him.\",1],\n",
    "            [\"NAH Sometimes timing sucks and just can't be helped. You were ready to move on with a new husband. That's okay. Your daughter wasn't ready for a new father, or a stepfather. That's okay. Your husband did his best. That's good. Your daughters grown up, and is ready to be more mature about things. That's good. Your husband has been hurt in the past, and is reluctant to take more emotional risks. That's okay. While time doesn't heal all wounds, time may help here. One can't see the future, but one can have hope.\",1],\n",
    "            [\"INFO: If your 22 year old fianc√©e disagrees with you, would you ground her too?\",1]\n",
    "            ]\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YTA: 2\n",
      "NTA: 3\n",
      "INFO: 1\n",
      "ESH: 1\n",
      "NAH: 1\n",
      "weighted_YTA: 2\n",
      "weighted_NTA: 3\n",
      "weighted_INFO: 1\n",
      "weighted_ESH: 1\n",
      "weighted_NAH: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# NTA, NTA, INFO or NTA, YTA, YTA, ESH, NAH, INFO\n",
    "\n",
    "#comments = [[\"YTA\", 1],[\"NTA\", 2], [\"NAH\", 3],[\"ESH\", 4],]\n",
    "vote_df = pd.DataFrame(comments,columns=[\"comment_text\", \"comment_score\"])\n",
    "\n",
    "#print(vote_df)\n",
    "extracted = get_judgement_labels(vote_df)\n",
    "\n",
    "for s,v in extracted:\n",
    "    print(f\"{s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
